{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Percona Distribution for PostgreSQL 14 Documentation \u00b6 Percona Distribution for PostgreSQL is a collection of tools to assist you in managing your PostgreSQL database system: it installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently: pg_repack rebuilds PostgreSQL database objects pgAudit provides detailed session or object audit logging via the standard PostgreSQL logging facility pgAudit set_user - The set_user part of pgAudit extension provides an additional layer of logging and control when unprivileged users must escalate themselves to superuser or object owner roles in order to perform needed maintenance tasks. pgBackRest is a backup and restore solution for PostgreSQL Patroni is an HA (High Availability) solution for PostgreSQL. pg_stat_monitor collects and aggregates statistics for PostgreSQL and provides histogram information. PgBouncer - a lightweight connection pooler for PostgreSQL pgBadger - a fast PostgreSQL Log Analyzer. wal2json - a PostgreSQL logical decoding JSON output plugin. A collection of additional PostgreSQL contrib extensions Seealso Blog Posts pgBackRest - A Great Backup Solution and a Wonderful Year of Growth Securing PostgreSQL as an Enterprise-Grade Environment Percona Distribution for PostgreSQL is also shipped with the libpq library. It contains \u201ca set of library functions that allow client programs to pass queries to the PostgreSQL backend server and to receive the results of these queries.\u201d 1 Installation and Upgrade \u00b6 Installing Percona Distribution for PostgreSQL Minor Upgrade of Percona Distribution for PostgreSQL Extensions \u00b6 pg_stat_monitor Solutions \u00b6 High Availability in PostgreSQL with Patroni Deploying high-availability on Debian and Ubuntu Deploying high-availability on RHEL and CentOS Testing the Patroni PostgreSQL Cluster Backup and disaster recovery with pgBackRest Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL Uninstall \u00b6 Uninstalling Percona Distribution for PostgreSQL Release Notes \u00b6 Release notes Reference \u00b6 Licensing https://www.postgresql.org/docs/14/libpq.html \u21a9","title":"Percona Distribution for PostgreSQL 14 Documentation"},{"location":"index.html#percona-distribution-for-postgresql-14-documentation","text":"Percona Distribution for PostgreSQL is a collection of tools to assist you in managing your PostgreSQL database system: it installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently: pg_repack rebuilds PostgreSQL database objects pgAudit provides detailed session or object audit logging via the standard PostgreSQL logging facility pgAudit set_user - The set_user part of pgAudit extension provides an additional layer of logging and control when unprivileged users must escalate themselves to superuser or object owner roles in order to perform needed maintenance tasks. pgBackRest is a backup and restore solution for PostgreSQL Patroni is an HA (High Availability) solution for PostgreSQL. pg_stat_monitor collects and aggregates statistics for PostgreSQL and provides histogram information. PgBouncer - a lightweight connection pooler for PostgreSQL pgBadger - a fast PostgreSQL Log Analyzer. wal2json - a PostgreSQL logical decoding JSON output plugin. A collection of additional PostgreSQL contrib extensions Seealso Blog Posts pgBackRest - A Great Backup Solution and a Wonderful Year of Growth Securing PostgreSQL as an Enterprise-Grade Environment Percona Distribution for PostgreSQL is also shipped with the libpq library. It contains \u201ca set of library functions that allow client programs to pass queries to the PostgreSQL backend server and to receive the results of these queries.\u201d 1","title":"Percona Distribution for PostgreSQL 14 Documentation"},{"location":"index.html#installation-and-upgrade","text":"Installing Percona Distribution for PostgreSQL Minor Upgrade of Percona Distribution for PostgreSQL","title":"Installation and Upgrade"},{"location":"index.html#extensions","text":"pg_stat_monitor","title":"Extensions"},{"location":"index.html#solutions","text":"High Availability in PostgreSQL with Patroni Deploying high-availability on Debian and Ubuntu Deploying high-availability on RHEL and CentOS Testing the Patroni PostgreSQL Cluster Backup and disaster recovery with pgBackRest Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL","title":"Solutions"},{"location":"index.html#uninstall","text":"Uninstalling Percona Distribution for PostgreSQL","title":"Uninstall"},{"location":"index.html#release-notes","text":"Release notes","title":"Release Notes"},{"location":"index.html#reference","text":"Licensing https://www.postgresql.org/docs/14/libpq.html \u21a9","title":"Reference"},{"location":"extensions.html","text":"Extensions \u00b6 pg_stat_monitor","title":"Extensions"},{"location":"extensions.html#extensions","text":"pg_stat_monitor","title":"Extensions"},{"location":"installation-and-update.html","text":"Installation and Upgrade \u00b6 This section provides instructions on how to: install Percona Distribution for PostgreSQL 14; update Percona Distribution for PostgreSQL to the latest minor version; upgrade Percona Distribution for PostgreSQL to a new major version.","title":"Installation and Upgrade"},{"location":"installation-and-update.html#installation-and-upgrade","text":"This section provides instructions on how to: install Percona Distribution for PostgreSQL 14; update Percona Distribution for PostgreSQL to the latest minor version; upgrade Percona Distribution for PostgreSQL to a new major version.","title":"Installation and Upgrade"},{"location":"installing.html","text":"Installing Percona Distribution for PostgreSQL \u00b6 Percona provides installation packages in DEB and RPM format for 64-bit Linux distributions. Find the full list of supported platforms on the Percona Software and Platform Lifecycle page . Like many other Percona products, we recommend installing Percona Distribution for PostgreSQL from Percona repositories by using the percona-release utility. The percona-release utility automatically enables the required repository for you so you can easily install and update Percona Distribution for PostgreSQL packages and their dependencies through the package manager of your operating system. The installation process includes the following steps: Install percona-release Enable the repository Install the packages Start the postgresql service Connect to the server Repositories overview \u00b6 There are two repositories available for Percona Distribution for PostgreSQL. We recommend installing Percona Distribution for PostgreSQL from the Major Release repository (e.g. ppg-14 ) as it includes the latest version packages. Whenever a package is updated, the package manager of your operating system detects that and prompts you to update. As long as you update all Distribution packages at the same time, you can ensure that the packages you\u2019re using have been tested and verified by Percona. The Minor Release repository includes a particular minor release of the database and all of the packages that were tested and verified to work with that minor release (e.g. ppg-14.0 ). You may choose to install Percona Distribution for PostgreSQL from the Minor Release repository if you have decided to standardize on a particular release which has passed rigorous testing procedures and which has been verified to work with your applications. This allows you to deploy to a new host and ensure that you\u2019ll be using the same version of all the Distribution packages, even if newer releases exist in other repositories. The disadvantage of using a Minor Release repository is that you are locked in this particular release. When potentially critical fixes are released in a later minor version of the database, you will not be prompted for an upgrade by the package manager of your operating system. You would need to change the configured repository in order to install the upgrade. Install percona-release \u00b6 Install percona-release utility. If you have installed it before, update it to the latest version. Enable the repository \u00b6 As soon as percona-release is installed or up-to-date, enable the repository for Percona Distribution for PostgreSQL ( ppg-14 ). We recommend using the set up command as it enables the specified repository and updates the platform\u2019s package manager database. To install the latest version of Percona Distribution for PostgreSQL, enable the Major Release repository using the following command: $ sudo percona-release setup ppg-14 To install a specific minor version of Percona Distribution for PostgreSQL, enable the Minor release repository. For example, to install Percona Distribution for PostgreSQL 14.1, enable the ppg-14.1 repository using the following command: $ sudo percona-release setup ppg-14.1 Install Percona Distribution for PostgreSQL packages \u00b6 After you\u2019ve installed percona-release and enabled the desired repository, install Percona Distribution for PostgreSQL using the commands of your package manager (the procedure differs depending on the package manager of your operating system). On Debian and Ubuntu using apt \u00b6 Note On Debian and other systems that use the apt package manager, such as Ubuntu, components of Percona Distribution for PostgreSQL 14 can only be installed together with the server shipped by Percona (percona-postgresql-14). If you wish to use Percona Distribution for PostgreSQL, uninstall the PostgreSQL package provided by your distribution (postgresql-14) and then install the chosen components from Percona Distribution for PostgreSQL. Install the percona-postgresql-14 package using apt install . $ sudo apt install percona-postgresql-14 Note that this package will not install the components. Use the following commands to install components\u2019 packages: Install pg_repack : $ sudo apt install percona-postgresql-14-repack Install pgAudit : $ sudo apt install percona-postgresql-14-pgaudit Install pgBackRest : $ sudo apt install percona-pgbackrest Install Patroni : $ sudo apt install percona-patroni Install pg_stat_monitor Install pgBouncer : $ sudo apt install percona-pgbouncer Install pgAudit-set_user : $ sudo apt install percona-pgaudit14-set-user Install pgBadger : $ sudo apt install percona-pgbadger Install wal2json : $ sudo apt install percona-postgresql-14-wal2json Install PostgreSQL contrib extensions: $ sudo apt install percona-postgresql-contrib Some extensions require additional setup in order to use them with Percona Distribution for PostgreSQL. For more information, refer to Enabling extensions . Starting the service \u00b6 The installation process automatically initializes the default database. Thus, to start Percona Distribution for PostgreSQL, use the following command: $ sudo pg_ctlcluster 14 main start Next steps: connect to PostgreSQL . On Red Hat Enterprise Linux and CentOS using yum \u00b6 Platform Specific Notes \u00b6 If you intend to install Percona Distribution for PostgreSQL on Red Hat Enterprise Linux v8 / CentOS 8, disable the postgresql and llvm-toolset modules: $ sudo dnf module disable postgresql llvm-toolset On CentOS 7, you should install the epel-release package: $ sudo yum -y install epel-release $ sudo yum repolist Install the percona-postgresql-14 package using yum install . $ sudo yum install percona-postgresql14-server Note that this package will not install the components. Use the following commands to install components\u2019 packages: Install pg_repack : $ sudo yum install percona-pg_repack14 Install pgaudit : $ sudo yum install percona-pgaudit Install pgBackRest : $ sudo yum install percona-pgbackrest Install Patroni : $ sudo yum install percona-patroni Install pg_stat_monitor : Install pgBouncer : $ sudo yum install percona-pgbouncer Install pgAudit-set_user : $ sudo yum install percona-pgaudit14_set_user Install pgBadger : $ sudo yum install percona-pgbadger Install wal2json : $ sudo yum install percona-wal2json14 Install PostgreSQL contrib extensions: $ sudo yum install percona-postgresql14-contrib Some extensions require additional setup in order to use them with Percona Distribution for PostgreSQL. For more information, refer to Enabling extensions . Starting the service \u00b6 After the installation, the default database storage is not automatically initialized. To complete the installation and start Percona Distribution for PostgreSQL, initialize the database using the following command: /usr/pgsql-14/bin/postgresql-14-setup initdb Start the PostgreSQL service: $ sudo systemctl start postgresql-14 Enabling extensions \u00b6 Some extensions require additional configuration before using them with Percona Distribution for PostgreSQL. This sections provides configuration instructions per extension. Patroni While setting up a high availability PostgreSQL cluster with Patroni, you will need the following: Configure Patroni on every postresql instance. The configuration file is supplied with percona-patroni package and is available at the following path: /etc/postgresql.yml for Debian and Ubuntu /usr/share/doc/percona-patroni/postgres0.yml for Red Hat Enterprise Linux and CentOS Install and configure Distributed Configuration Store (DCS). Patroni supports such DCSs as ETCD, zookeeper, Kubernetes though ETCD is the most popular one. It is available upstream as DEB packages for Debian 10 and Ubuntu 18.04, 20.04. For Debian 9 (\u201cstretch\u201d), a DEB package for ETCD is available within Percona Distribution for PostreSQL. You can install it using the following command: $ apt install etcd For CentOS 8, RPM packages for ETCD is available within Percona Distribution for PostreSQL. You can install it using the following command: $ yum install etcd python3-python-etcd Install and configure HAProxy . Seealso Patroni documentation Percona Blog: PostgreSQL HA with Patroni: Your Turn to Test Failure Scenarios pgBadger Enable the following options in postgresql.conf configuration file before starting the service: log_min_duration_statement = 0 log_line_prefix = '%t [%p]: ' log_checkpoints = on log_connections = on log_disconnections = on log_lock_waits = on log_temp_files = 0 log_autovacuum_min_duration = 0 log_error_verbosity = default For details about each option, see pdBadger documentation . pgAudit set-user Add the set-user to shared_preload_libraries in postgresql.conf . The recommended way is to use the ALTER SYSTEM command. Connect to psql and use the following command: $ ALTER SYSTEM SET shared_preload_libraries = 'set-user'; Start / restart the server to apply the configuration. You can fine-tune user behavior with the custom parameters supplied with the extension. wal2json After the installation, enable the following option in postgresql.conf configuration file before starting the service: wal_level = logical Connect to the PostgreSQL server \u00b6 By default, postgres user and postgres database are created in PostgreSQL upon its installation and initialization. This allows you to connect to the database as the postgres user. $ sudo su postgres Open the PostgreSQL interactive terminal: $ psql Hint You can connect to psql as the postgres user in one go: $ sudo su postgres psql To exit the psql terminal, use the following command: $ \\q","title":"Installing Percona Distribution for PostgreSQL"},{"location":"installing.html#installing-percona-distribution-for-postgresql","text":"Percona provides installation packages in DEB and RPM format for 64-bit Linux distributions. Find the full list of supported platforms on the Percona Software and Platform Lifecycle page . Like many other Percona products, we recommend installing Percona Distribution for PostgreSQL from Percona repositories by using the percona-release utility. The percona-release utility automatically enables the required repository for you so you can easily install and update Percona Distribution for PostgreSQL packages and their dependencies through the package manager of your operating system. The installation process includes the following steps: Install percona-release Enable the repository Install the packages Start the postgresql service Connect to the server","title":"Installing Percona Distribution for PostgreSQL"},{"location":"installing.html#repositories-overview","text":"There are two repositories available for Percona Distribution for PostgreSQL. We recommend installing Percona Distribution for PostgreSQL from the Major Release repository (e.g. ppg-14 ) as it includes the latest version packages. Whenever a package is updated, the package manager of your operating system detects that and prompts you to update. As long as you update all Distribution packages at the same time, you can ensure that the packages you\u2019re using have been tested and verified by Percona. The Minor Release repository includes a particular minor release of the database and all of the packages that were tested and verified to work with that minor release (e.g. ppg-14.0 ). You may choose to install Percona Distribution for PostgreSQL from the Minor Release repository if you have decided to standardize on a particular release which has passed rigorous testing procedures and which has been verified to work with your applications. This allows you to deploy to a new host and ensure that you\u2019ll be using the same version of all the Distribution packages, even if newer releases exist in other repositories. The disadvantage of using a Minor Release repository is that you are locked in this particular release. When potentially critical fixes are released in a later minor version of the database, you will not be prompted for an upgrade by the package manager of your operating system. You would need to change the configured repository in order to install the upgrade.","title":"Repositories overview"},{"location":"installing.html#install-percona-release","text":"Install percona-release utility. If you have installed it before, update it to the latest version.","title":"Install percona-release"},{"location":"installing.html#enable-the-repository","text":"As soon as percona-release is installed or up-to-date, enable the repository for Percona Distribution for PostgreSQL ( ppg-14 ). We recommend using the set up command as it enables the specified repository and updates the platform\u2019s package manager database. To install the latest version of Percona Distribution for PostgreSQL, enable the Major Release repository using the following command: $ sudo percona-release setup ppg-14 To install a specific minor version of Percona Distribution for PostgreSQL, enable the Minor release repository. For example, to install Percona Distribution for PostgreSQL 14.1, enable the ppg-14.1 repository using the following command: $ sudo percona-release setup ppg-14.1","title":"Enable the repository"},{"location":"installing.html#install-percona-distribution-for-postgresql-packages","text":"After you\u2019ve installed percona-release and enabled the desired repository, install Percona Distribution for PostgreSQL using the commands of your package manager (the procedure differs depending on the package manager of your operating system).","title":"Install Percona Distribution for PostgreSQL packages"},{"location":"installing.html#on-debian-and-ubuntu-using-apt","text":"Note On Debian and other systems that use the apt package manager, such as Ubuntu, components of Percona Distribution for PostgreSQL 14 can only be installed together with the server shipped by Percona (percona-postgresql-14). If you wish to use Percona Distribution for PostgreSQL, uninstall the PostgreSQL package provided by your distribution (postgresql-14) and then install the chosen components from Percona Distribution for PostgreSQL. Install the percona-postgresql-14 package using apt install . $ sudo apt install percona-postgresql-14 Note that this package will not install the components. Use the following commands to install components\u2019 packages: Install pg_repack : $ sudo apt install percona-postgresql-14-repack Install pgAudit : $ sudo apt install percona-postgresql-14-pgaudit Install pgBackRest : $ sudo apt install percona-pgbackrest Install Patroni : $ sudo apt install percona-patroni Install pg_stat_monitor Install pgBouncer : $ sudo apt install percona-pgbouncer Install pgAudit-set_user : $ sudo apt install percona-pgaudit14-set-user Install pgBadger : $ sudo apt install percona-pgbadger Install wal2json : $ sudo apt install percona-postgresql-14-wal2json Install PostgreSQL contrib extensions: $ sudo apt install percona-postgresql-contrib Some extensions require additional setup in order to use them with Percona Distribution for PostgreSQL. For more information, refer to Enabling extensions .","title":"On Debian and Ubuntu using apt"},{"location":"installing.html#starting-the-service","text":"The installation process automatically initializes the default database. Thus, to start Percona Distribution for PostgreSQL, use the following command: $ sudo pg_ctlcluster 14 main start Next steps: connect to PostgreSQL .","title":"Starting the service"},{"location":"installing.html#on-red-hat-enterprise-linux-and-centos-using-yum","text":"","title":"On Red Hat Enterprise Linux and CentOS using yum"},{"location":"installing.html#platform-specific-notes","text":"If you intend to install Percona Distribution for PostgreSQL on Red Hat Enterprise Linux v8 / CentOS 8, disable the postgresql and llvm-toolset modules: $ sudo dnf module disable postgresql llvm-toolset On CentOS 7, you should install the epel-release package: $ sudo yum -y install epel-release $ sudo yum repolist Install the percona-postgresql-14 package using yum install . $ sudo yum install percona-postgresql14-server Note that this package will not install the components. Use the following commands to install components\u2019 packages: Install pg_repack : $ sudo yum install percona-pg_repack14 Install pgaudit : $ sudo yum install percona-pgaudit Install pgBackRest : $ sudo yum install percona-pgbackrest Install Patroni : $ sudo yum install percona-patroni Install pg_stat_monitor : Install pgBouncer : $ sudo yum install percona-pgbouncer Install pgAudit-set_user : $ sudo yum install percona-pgaudit14_set_user Install pgBadger : $ sudo yum install percona-pgbadger Install wal2json : $ sudo yum install percona-wal2json14 Install PostgreSQL contrib extensions: $ sudo yum install percona-postgresql14-contrib Some extensions require additional setup in order to use them with Percona Distribution for PostgreSQL. For more information, refer to Enabling extensions .","title":"Platform Specific Notes"},{"location":"installing.html#starting-the-service_1","text":"After the installation, the default database storage is not automatically initialized. To complete the installation and start Percona Distribution for PostgreSQL, initialize the database using the following command: /usr/pgsql-14/bin/postgresql-14-setup initdb Start the PostgreSQL service: $ sudo systemctl start postgresql-14","title":"Starting the service"},{"location":"installing.html#enabling-extensions","text":"Some extensions require additional configuration before using them with Percona Distribution for PostgreSQL. This sections provides configuration instructions per extension. Patroni While setting up a high availability PostgreSQL cluster with Patroni, you will need the following: Configure Patroni on every postresql instance. The configuration file is supplied with percona-patroni package and is available at the following path: /etc/postgresql.yml for Debian and Ubuntu /usr/share/doc/percona-patroni/postgres0.yml for Red Hat Enterprise Linux and CentOS Install and configure Distributed Configuration Store (DCS). Patroni supports such DCSs as ETCD, zookeeper, Kubernetes though ETCD is the most popular one. It is available upstream as DEB packages for Debian 10 and Ubuntu 18.04, 20.04. For Debian 9 (\u201cstretch\u201d), a DEB package for ETCD is available within Percona Distribution for PostreSQL. You can install it using the following command: $ apt install etcd For CentOS 8, RPM packages for ETCD is available within Percona Distribution for PostreSQL. You can install it using the following command: $ yum install etcd python3-python-etcd Install and configure HAProxy . Seealso Patroni documentation Percona Blog: PostgreSQL HA with Patroni: Your Turn to Test Failure Scenarios pgBadger Enable the following options in postgresql.conf configuration file before starting the service: log_min_duration_statement = 0 log_line_prefix = '%t [%p]: ' log_checkpoints = on log_connections = on log_disconnections = on log_lock_waits = on log_temp_files = 0 log_autovacuum_min_duration = 0 log_error_verbosity = default For details about each option, see pdBadger documentation . pgAudit set-user Add the set-user to shared_preload_libraries in postgresql.conf . The recommended way is to use the ALTER SYSTEM command. Connect to psql and use the following command: $ ALTER SYSTEM SET shared_preload_libraries = 'set-user'; Start / restart the server to apply the configuration. You can fine-tune user behavior with the custom parameters supplied with the extension. wal2json After the installation, enable the following option in postgresql.conf configuration file before starting the service: wal_level = logical","title":"Enabling extensions"},{"location":"installing.html#connect-to-the-postgresql-server","text":"By default, postgres user and postgres database are created in PostgreSQL upon its installation and initialization. This allows you to connect to the database as the postgres user. $ sudo su postgres Open the PostgreSQL interactive terminal: $ psql Hint You can connect to psql as the postgres user in one go: $ sudo su postgres psql To exit the psql terminal, use the following command: $ \\q","title":"Connect to the PostgreSQL server"},{"location":"licensing.html","text":"Licensing \u00b6 Percona Distribution for PostgreSQL is licensed under the PostgreSQL license and licenses of all components included in the Distribution. Documentation Licensing \u00b6 Percona Distribution for PostgreSQL documentation is licensed under the PostgreSQL license .","title":"Licensing"},{"location":"licensing.html#licensing","text":"Percona Distribution for PostgreSQL is licensed under the PostgreSQL license and licenses of all components included in the Distribution.","title":"Licensing"},{"location":"licensing.html#documentation-licensing","text":"Percona Distribution for PostgreSQL documentation is licensed under the PostgreSQL license .","title":"Documentation Licensing"},{"location":"major-upgrade.html","text":"Upgrading Percona Distribution for PostgreSQL from 13 to 14 \u00b6 This document describes the in-place upgrade of Percona Distribution for PostgreSQL using the pg_upgrade tool. The in-place upgrade means installing a new version without removing the old version and keeping the data files on the server. Seealso pg_upgrade Documentation Similar to installing, we recommend you to upgrade Percona Distribution for PostgreSQL from Percona repositories. Important A major upgrade is a risky process because of many changes between versions and issues that might occur during or after the upgrade. Therefore, make sure to back up your data first. The backup tools are out of scope of this document. Use the backup tool of your choice. The general in-place upgrade flow for Percona Distribution for PostgreSQL is the following: Install Percona Distribution for PostgreSQL 14 packages. Stop the PostgreSQL service. Check the upgrade without modifying the data. Upgrade Percona Distribution for PostgreSQL. Start PostgreSQL service. Execute the analyze_new_cluster.sh script to generate statistics so the system is usable. Delete old packages and configuration files. The exact steps may differ depending on the package manager of your operating system. On Debian and Ubuntu using apt \u00b6 Important Run all commands as root or via sudo . Install Percona Distribution for PostgreSQL 14 packages. Enable Percona repository using the percona-release utility: $ sudo percona-release setup ppg-14 Install Percona Distribution for PostgreSQL 14 package: $ sudo apt install percona-postgresql-14 Install the components: $ sudo apt install percona-postgresql-14-repack \\ percona-postgresql-14-pgaudit \\ percona-pgbackrest \\ percona-patroni \\ percona-pgbadger \\ percona-pgaudit14-set-user \\ percona-pgbadger \\ percona-postgresql-14-wal2json \\ percona-pg-stat-monitor14 \\ percona-postgresql-contrib Seealso Percona Documentation: Percona Software Repositories Documentation Installing Percona Distribution for PostgreSQL Stop the postgresql service. $ sudo systemctl stop postgresql.service This stops both Percona Distribution for PostgreSQL 13 and 14. Run the database upgrade. Log in as the postgres user. $ sudo su postgres Change the current directory to the tmp directory where logs and some scripts will be recorded: cd tmp/ Check the ability to upgrade Percona Distribution for PostgreSQL from 13 to 14: $ /usr/lib/postgresql/14/bin/pg_upgrade --old-datadir=/var/lib/postgresql/13/main \\ --new-datadir=/var/lib/postgresql/14/main \\ --old-bindir=/usr/lib/postgresql/13/bin \\ --new-bindir=/usr/lib/postgresql/14/bin \\ --old-options '-c config_file=/etc/postgresql/13/main/postgresql.conf' \\ --new-options '-c config_file=/etc/postgresql/14/main/postgresql.conf' \\ --check The --check flag here instructs pg_upgrade to only check the upgrade without changing any data. Sample output Performing Consistency Checks ----------------------------- Checking cluster versions ok Checking database user is the install user ok Checking database connection settings ok Checking for prepared transactions ok Checking for reg* data types in user tables ok Checking for contrib/isn with bigint-passing mismatch ok Checking for tables WITH OIDS ok Checking for invalid \"sql_identifier\" user columns ok Checking for presence of required libraries ok Checking database user is the install user ok Checking for prepared transactions ok *Clusters are compatible* Upgrade the Percona Distribution for PostgreSQL $ /usr/lib/postgresql/14/bin/pg_upgrade --old-datadir=/var/lib/postgresql/13/main \\ --new-datadir=/var/lib/postgresql/14/main \\ --old-bindir=/usr/lib/postgresql/13/bin \\ --new-bindir=/usr/lib/postgresql/14/bin \\ --old-options '-c config_file=/etc/postgresql/13/main/postgresql.conf' \\ --new-options '-c config_file=/etc/postgresql/14/main/postgresql.conf' \\ --link The --link flag creates hard links to the files on the old version cluster so you don\u2019t need to copy data. If you don\u2019t wish to use the --link option, make sure that you have enough disk space to store 2 copies of files for both old version and new version clusters. Go back to the regular user: exit The Percona Distribution for PostgreSQL 13 uses the 5432 port while the Percona Distribution for PostgreSQL 14 is set up to use the 5433 port by default. To start the Percona Distribution for PostgreSQL 14, swap ports in the configuration files of both versions. $ sudo vim /etc/postgresql/14/main/postgresql.conf $ port = 5433 # Change to 5432 here $ sudo vim /etc/postgresql/13/main/postgresql.conf $ port = 5432 # Change to 5433 here Start the postgreqsl service. $ sudo systemctl start postgresql.service Check the postgresql version. Log in as a postgres user $ sudo su postgres Check the database version $ psql -c \"SELECT version();\" Run the analyze_new_cluster.sh script $ tmp/analyze_new_cluster.sh $ #Logout $ exit Delete Percona Distribution for PostgreSQL 13 packages and configuration files Remove packages $ sudo apt remove percona-postgresql-13* percona-pgbackrest percona-patroni percona-pg-stat-monitor13 percona-pgaudit13-set-user percona-pgbadger percona-pgbouncer percona-postgresql-13-wal2json Remove old files $ rm -rf /etc/postgresql/13/main On Red Hat Enterprise Linux and CentOS using yum \u00b6 Important Run all commands as root or via sudo . Install Percona Distribution for PostgreSQL 14 packages Enable Percona repository using the percona-release utility: $ sudo percona-release setup ppg-14 Install Percona Distribution for PostgreSQL 14: $ sudo yum install percona-postgresql14-server Install components: $ sudo yum install percona-pgaudit \\ percona-pgbackrest \\ percona-pg_repack14 \\ percona-patroni \\ percona-pg-stat-monitor14 \\ percona-pgbadger \\ percona-pgaudit14_set_user \\ percona-pgbadger \\ percona-wal2json14 \\ percona-postgresql14-contrib Seealso Percona Documentation: Percona Software Repositories Documentation Installing Percona Distribution for PostgreSQL Set up Percona Distribution for PostgreSQL 14 cluster Log is as the postgres user sudo su postgres Set up locale settings export LC_ALL=\"en_US.UTF-8\" export LC_CTYPE=\"en_US.UTF-8\" Initialize cluster with the new data directory /usr/pgsql-14/bin/initdb -D /var/lib/pgsql/13/data Stop the postgresql 13 service $ systemctl stop postgresql-13 Run the database upgrade. Log in as the postgres user $ sudo su postgres Check the ability to upgrade Percona Distribution for PostgreSQL from 13 to 14: $ /usr/pgsql-14/bin/pg_upgrade \\ --old-bindir /usr/pgsql-13/bin \\ --new-bindir /usr/pgsql-14/bin \\ --old-datadir /var/lib/pgsql/13/data \\ --new-datadir /var/lib/pgsql/14/data \\ --link --check The --check flag here instructs pg_upgrade to only check the upgrade without changing any data. Sample output Performing Consistency Checks ----------------------------- Checking cluster versions ok Checking database user is the install user ok Checking database connection settings ok Checking for prepared transactions ok Checking for reg* data types in user tables ok Checking for contrib/isn with bigint-passing mismatch ok Checking for tables WITH OIDS ok Checking for invalid \"sql_identifier\" user columns ok Checking for presence of required libraries ok Checking database user is the install user ok Checking for prepared transactions ok *Clusters are compatible* Upgrade the Percona Distribution for PostgreSQL $ /usr/pgsql-14/bin/pg_upgrade \\ --old-bindir /usr/pgsql-13/bin \\ --new-bindir /usr/pgsql-14/bin \\ --old-datadir /var/lib/pgsql/13/data \\ --new-datadir /var/lib/pgsql/14/data \\ --link The --link flag creates hard links to the files on the old version cluster so you don\u2019t need to copy data. If you don\u2019t wish to use the --link option, make sure that you have enough disk space to store 2 copies of files for both old version and new version clusters. Start the postgresql 14 service. $ systemctl start postgresql-14 Check postgresql status $ systemctl status postgresql-14 Run the analyze_new_cluster.sh script Log in as the postgres user $ sudo su postgres Run the script $ ./analyze_new_cluster.sh Delete Percona Distribution for PostgreSQL 13 configuration files $ ./delete_old_cluster.sh Delete Percona Distribution for PostgreSQL 13 packages Remove packages $ sudo yum -y remove percona-postgresql13* Remove old files $ rm -rf /var/lib/pgsql/13/data","title":"Upgrading Percona Distribution for PostgreSQL from 13 to 14"},{"location":"major-upgrade.html#upgrading-percona-distribution-for-postgresql-from-13-to-14","text":"This document describes the in-place upgrade of Percona Distribution for PostgreSQL using the pg_upgrade tool. The in-place upgrade means installing a new version without removing the old version and keeping the data files on the server. Seealso pg_upgrade Documentation Similar to installing, we recommend you to upgrade Percona Distribution for PostgreSQL from Percona repositories. Important A major upgrade is a risky process because of many changes between versions and issues that might occur during or after the upgrade. Therefore, make sure to back up your data first. The backup tools are out of scope of this document. Use the backup tool of your choice. The general in-place upgrade flow for Percona Distribution for PostgreSQL is the following: Install Percona Distribution for PostgreSQL 14 packages. Stop the PostgreSQL service. Check the upgrade without modifying the data. Upgrade Percona Distribution for PostgreSQL. Start PostgreSQL service. Execute the analyze_new_cluster.sh script to generate statistics so the system is usable. Delete old packages and configuration files. The exact steps may differ depending on the package manager of your operating system.","title":"Upgrading Percona Distribution for PostgreSQL from 13 to 14"},{"location":"major-upgrade.html#on-debian-and-ubuntu-using-apt","text":"Important Run all commands as root or via sudo . Install Percona Distribution for PostgreSQL 14 packages. Enable Percona repository using the percona-release utility: $ sudo percona-release setup ppg-14 Install Percona Distribution for PostgreSQL 14 package: $ sudo apt install percona-postgresql-14 Install the components: $ sudo apt install percona-postgresql-14-repack \\ percona-postgresql-14-pgaudit \\ percona-pgbackrest \\ percona-patroni \\ percona-pgbadger \\ percona-pgaudit14-set-user \\ percona-pgbadger \\ percona-postgresql-14-wal2json \\ percona-pg-stat-monitor14 \\ percona-postgresql-contrib Seealso Percona Documentation: Percona Software Repositories Documentation Installing Percona Distribution for PostgreSQL Stop the postgresql service. $ sudo systemctl stop postgresql.service This stops both Percona Distribution for PostgreSQL 13 and 14. Run the database upgrade. Log in as the postgres user. $ sudo su postgres Change the current directory to the tmp directory where logs and some scripts will be recorded: cd tmp/ Check the ability to upgrade Percona Distribution for PostgreSQL from 13 to 14: $ /usr/lib/postgresql/14/bin/pg_upgrade --old-datadir=/var/lib/postgresql/13/main \\ --new-datadir=/var/lib/postgresql/14/main \\ --old-bindir=/usr/lib/postgresql/13/bin \\ --new-bindir=/usr/lib/postgresql/14/bin \\ --old-options '-c config_file=/etc/postgresql/13/main/postgresql.conf' \\ --new-options '-c config_file=/etc/postgresql/14/main/postgresql.conf' \\ --check The --check flag here instructs pg_upgrade to only check the upgrade without changing any data. Sample output Performing Consistency Checks ----------------------------- Checking cluster versions ok Checking database user is the install user ok Checking database connection settings ok Checking for prepared transactions ok Checking for reg* data types in user tables ok Checking for contrib/isn with bigint-passing mismatch ok Checking for tables WITH OIDS ok Checking for invalid \"sql_identifier\" user columns ok Checking for presence of required libraries ok Checking database user is the install user ok Checking for prepared transactions ok *Clusters are compatible* Upgrade the Percona Distribution for PostgreSQL $ /usr/lib/postgresql/14/bin/pg_upgrade --old-datadir=/var/lib/postgresql/13/main \\ --new-datadir=/var/lib/postgresql/14/main \\ --old-bindir=/usr/lib/postgresql/13/bin \\ --new-bindir=/usr/lib/postgresql/14/bin \\ --old-options '-c config_file=/etc/postgresql/13/main/postgresql.conf' \\ --new-options '-c config_file=/etc/postgresql/14/main/postgresql.conf' \\ --link The --link flag creates hard links to the files on the old version cluster so you don\u2019t need to copy data. If you don\u2019t wish to use the --link option, make sure that you have enough disk space to store 2 copies of files for both old version and new version clusters. Go back to the regular user: exit The Percona Distribution for PostgreSQL 13 uses the 5432 port while the Percona Distribution for PostgreSQL 14 is set up to use the 5433 port by default. To start the Percona Distribution for PostgreSQL 14, swap ports in the configuration files of both versions. $ sudo vim /etc/postgresql/14/main/postgresql.conf $ port = 5433 # Change to 5432 here $ sudo vim /etc/postgresql/13/main/postgresql.conf $ port = 5432 # Change to 5433 here Start the postgreqsl service. $ sudo systemctl start postgresql.service Check the postgresql version. Log in as a postgres user $ sudo su postgres Check the database version $ psql -c \"SELECT version();\" Run the analyze_new_cluster.sh script $ tmp/analyze_new_cluster.sh $ #Logout $ exit Delete Percona Distribution for PostgreSQL 13 packages and configuration files Remove packages $ sudo apt remove percona-postgresql-13* percona-pgbackrest percona-patroni percona-pg-stat-monitor13 percona-pgaudit13-set-user percona-pgbadger percona-pgbouncer percona-postgresql-13-wal2json Remove old files $ rm -rf /etc/postgresql/13/main","title":"On Debian and Ubuntu using apt"},{"location":"major-upgrade.html#on-red-hat-enterprise-linux-and-centos-using-yum","text":"Important Run all commands as root or via sudo . Install Percona Distribution for PostgreSQL 14 packages Enable Percona repository using the percona-release utility: $ sudo percona-release setup ppg-14 Install Percona Distribution for PostgreSQL 14: $ sudo yum install percona-postgresql14-server Install components: $ sudo yum install percona-pgaudit \\ percona-pgbackrest \\ percona-pg_repack14 \\ percona-patroni \\ percona-pg-stat-monitor14 \\ percona-pgbadger \\ percona-pgaudit14_set_user \\ percona-pgbadger \\ percona-wal2json14 \\ percona-postgresql14-contrib Seealso Percona Documentation: Percona Software Repositories Documentation Installing Percona Distribution for PostgreSQL Set up Percona Distribution for PostgreSQL 14 cluster Log is as the postgres user sudo su postgres Set up locale settings export LC_ALL=\"en_US.UTF-8\" export LC_CTYPE=\"en_US.UTF-8\" Initialize cluster with the new data directory /usr/pgsql-14/bin/initdb -D /var/lib/pgsql/13/data Stop the postgresql 13 service $ systemctl stop postgresql-13 Run the database upgrade. Log in as the postgres user $ sudo su postgres Check the ability to upgrade Percona Distribution for PostgreSQL from 13 to 14: $ /usr/pgsql-14/bin/pg_upgrade \\ --old-bindir /usr/pgsql-13/bin \\ --new-bindir /usr/pgsql-14/bin \\ --old-datadir /var/lib/pgsql/13/data \\ --new-datadir /var/lib/pgsql/14/data \\ --link --check The --check flag here instructs pg_upgrade to only check the upgrade without changing any data. Sample output Performing Consistency Checks ----------------------------- Checking cluster versions ok Checking database user is the install user ok Checking database connection settings ok Checking for prepared transactions ok Checking for reg* data types in user tables ok Checking for contrib/isn with bigint-passing mismatch ok Checking for tables WITH OIDS ok Checking for invalid \"sql_identifier\" user columns ok Checking for presence of required libraries ok Checking database user is the install user ok Checking for prepared transactions ok *Clusters are compatible* Upgrade the Percona Distribution for PostgreSQL $ /usr/pgsql-14/bin/pg_upgrade \\ --old-bindir /usr/pgsql-13/bin \\ --new-bindir /usr/pgsql-14/bin \\ --old-datadir /var/lib/pgsql/13/data \\ --new-datadir /var/lib/pgsql/14/data \\ --link The --link flag creates hard links to the files on the old version cluster so you don\u2019t need to copy data. If you don\u2019t wish to use the --link option, make sure that you have enough disk space to store 2 copies of files for both old version and new version clusters. Start the postgresql 14 service. $ systemctl start postgresql-14 Check postgresql status $ systemctl status postgresql-14 Run the analyze_new_cluster.sh script Log in as the postgres user $ sudo su postgres Run the script $ ./analyze_new_cluster.sh Delete Percona Distribution for PostgreSQL 13 configuration files $ ./delete_old_cluster.sh Delete Percona Distribution for PostgreSQL 13 packages Remove packages $ sudo yum -y remove percona-postgresql13* Remove old files $ rm -rf /var/lib/pgsql/13/data","title":"On Red Hat Enterprise Linux and CentOS using yum"},{"location":"minor-upgrade.html","text":"Minor Upgrade of Percona Distribution for PostgreSQL \u00b6 Minor releases of PostgreSQL include bug fixes and feature enhancements. We recommend that you keep your Percona Distribution for PostgreSQL updated to the latest minor version. Though minor upgrades do not change the behavior, we recommend you to back up your data first, in order to be on the safe side. Minor upgrade of Percona Distribution for PostgreSQL includes the following steps: Stopping the postgresql cluster; Installing new version packages; Restarting the postgresql cluster. Note These steps apply if you installed Percona Distribution for PostgreSQL from the Major Release repository. In this case, you are always upgraded to the latest available release. If you installed Percona Distribution for PostgreSQL from the Minor Release repository, you will need to enable a new version repository to upgrade. For more information about Percona repositories, refer to Installing Percona Distribution for PostgreSQL. Before the upgrade, update the percona-release utility to the latest version. This is required to install the new version packages of Percona Distribution for PostgreSQL. Refer to Percona Software Repositories Documentation for update instructions. Important Run all commands as root or via sudo . Stop the postgresql service. On Debian / Ubuntu: $ sudo systemctl stop postgresql.service On Red Hat Enterprise Linux / CentOS: $ sudo systemctl stop postgresql-14 Install new version packages. See Installing Percona Distribution for PostgreSQL . Restart the postgresql service. On Debian / Ubuntu: $ sudo systemctl start postgresql.service On Red Hat Enterprise Linux / CentOS: $ sudo systemctl start postgresql-14 If you wish to upgrade Percona Distribution for PostgreSQL to the major version, refer to Upgrading Percona Distribution for PostgreSQL from 13 to 14 .","title":"Minor Upgrade of Percona Distribution for PostgreSQL"},{"location":"minor-upgrade.html#minor-upgrade-of-percona-distribution-for-postgresql","text":"Minor releases of PostgreSQL include bug fixes and feature enhancements. We recommend that you keep your Percona Distribution for PostgreSQL updated to the latest minor version. Though minor upgrades do not change the behavior, we recommend you to back up your data first, in order to be on the safe side. Minor upgrade of Percona Distribution for PostgreSQL includes the following steps: Stopping the postgresql cluster; Installing new version packages; Restarting the postgresql cluster. Note These steps apply if you installed Percona Distribution for PostgreSQL from the Major Release repository. In this case, you are always upgraded to the latest available release. If you installed Percona Distribution for PostgreSQL from the Minor Release repository, you will need to enable a new version repository to upgrade. For more information about Percona repositories, refer to Installing Percona Distribution for PostgreSQL. Before the upgrade, update the percona-release utility to the latest version. This is required to install the new version packages of Percona Distribution for PostgreSQL. Refer to Percona Software Repositories Documentation for update instructions. Important Run all commands as root or via sudo . Stop the postgresql service. On Debian / Ubuntu: $ sudo systemctl stop postgresql.service On Red Hat Enterprise Linux / CentOS: $ sudo systemctl stop postgresql-14 Install new version packages. See Installing Percona Distribution for PostgreSQL . Restart the postgresql service. On Debian / Ubuntu: $ sudo systemctl start postgresql.service On Red Hat Enterprise Linux / CentOS: $ sudo systemctl start postgresql-14 If you wish to upgrade Percona Distribution for PostgreSQL to the major version, refer to Upgrading Percona Distribution for PostgreSQL from 13 to 14 .","title":"Minor Upgrade of Percona Distribution for PostgreSQL"},{"location":"pg-stat-monitor.html","text":"pg_stat_monitor \u00b6 Note This document describes the functionality of pg_stat_monitor 1.0.0. Overview \u00b6 pg_stat_monitor is a Query Performance Monitoring tool for PostgreSQL. It collects various statistics data such as query statistics, query plan, SQL comments and other performance insights. The collected data is aggregated and presented in a single view. This allows you to view queries from performance, application and analysis perspectives. pg_stat_monitor groups statistics data and writes it in a storage unit called bucket . The data is added and stored in a bucket for the defined period \u2013 the bucket lifetime. This allows you to identify performance issues and patterns based on time. You can specify the following: The number of buckets. Together they form a bucket chain. Bucket size. This is the amount of shared memory allocated for buckets. Memory is divided equally among buckets. Bucket lifetime. When a bucket lifetime expires, pg_stat_monitor resets all statistics and writes the data in the next bucket in the chain. When the last bucket\u2019s lifetime expires, pg_stat_monitor returns to the first bucket. Important The contents of the bucket will be overwritten. In order not to lose the data, make sure to read the bucket before pg_stat_monitor starts writing new data to it. Views \u00b6 pg_stat_monitor provides two views: pg_stat_monitor is the view where statistics data is presented. pg_stat_monitor_settings view shows available configuration options which you can change. pg_stat_monitor view \u00b6 The pg_stat_monitor view contains all the statistics collected and aggregated by the extension. This view contains one row for each distinct combination of metrics and whether it is a top-level statement or not (up to the maximum number of distinct statements that the module can track). For details about available metrics, refer to the pg_stat_monitor view reference . The following are the primary keys for pg_stat_monitor: bucket , userid , dbid , client_ip , application_name . A new row is created for each key in the pg_stat_monitor view. For security reasons, only superusers and members of the pg_read_all_stats role are allowed to see the SQL text and queryid of queries executed by other users. Other users can see the statistics, however, if the view has been installed in their database. pg_stat_monitor_settings view \u00b6 The pg_stat_monitor_settings view shows one row per pg_stat_monitor configuration parameter. It displays configuration parameter name, value, default value, description, minimum and maximum values, and whether a restart is required for a change in value to be effective. To learn more, see the Changing the configuration section. Installation \u00b6 This section describes how to install pg_stat_monitor from Percona repositories. To learn about other installation methods, see the Installation section in the pg_stat_monitor documentation. Assumptions : We assume that you have installed percona-release utility and enabled the Percona Distribution for PostgreSQL repository To install pg_stat_monitor , run the following command: On Debian and Ubuntu: $ sudo apt install percona-pg-stat-monitor14 On Red Hat Enterprise Linux and CentOS: $ sudo yum install percona-pg-stat-monitor14 Setup \u00b6 pg_stat_monitor requires additional setup in order to use it with PostgreSQL. The setup steps are the following: Add pg_stat_monitor in the shared_preload_libraries configuration parameter. The recommended way to modify PostgreSQL configuration file is using the ALTER SYSTEM command. Connect to psql and use the following command: $ ALTER SYSTEM SET shared_preload_libraries = 'pg_stat_monitor'; The parameter value is written to the postgresql.auto.conf file which is read in addition with postgresql.conf file. Note If you\u2019ve added other values to the shared_preload_libraries parameter, list all of them separated by commas for the ALTER SYSTEM command. For example, ALTER SYSTEM SET shared_preload_libraries = \u2018foo, bar, pg_stat_monitor\u2019 . Start or restart the postgresql instance to enable pg_stat_monitor . Use the following command for restart: On Debian and Ubuntu: $ sudo systemctl restart postgresql.service On Red Hat Enterprise Linux and CentOS: $ sudo systemctl restart postgresql-14 Create the extension. Connect to psql and use the following command: $ CREATE EXTENSION pg_stat_monitor; Note By default, the extension is created against the postgres database. You need to create the extension on every database where you want to collect statistics. Tip To check the version of the extension, run the following command in the psql session: SELECT pg_stat_monitor_version () ; Usage \u00b6 For example, to view the IP address of the client application that made the query, run the following command: SELECT DISTINCT userid::regrole, pg_stat_monitor.datname, substr(query,0, 50) AS query, calls, bucket, bucket_start_time, queryid, client_ip FROM pg_stat_monitor, pg_database WHERE pg_database.oid = oid; userid | datname | query | calls | client_ip ----------+----------+---------------------------------------------------+-------+----------- postgres | postgres | select bucket, bucket_start_time, query,calls fro | 1 | 127.0.0.1 postgres | postgres | SELECT c.relchecks, c.relkind, c.relhasindex, c.r | 1 | 127.0.0.1 postgres | postgres | SELECT userid, total_time, min_time, max_time, | 1 | 127.0.0.1 Find more usage examples in the pg_stat_monitor User Guide . Changing the configuration \u00b6 Run the following query to list available configuration parameters. $ SELECT name,description FROM pg_stat_monitor_settings; Output name | description -----------------------------------------------+------------------------------------------------------------------- pg_stat_monitor.pgsm_max | Sets the maximum number of statements tracked by pg_stat_monitor. pg_stat_monitor.pgsm_query_max_len | Sets the maximum length of query. pg_stat_monitor.pgsm_enable | Enable/Disable statistics collector. pg_stat_monitor.pgsm_track_utility | Selects whether utility commands are tracked. pg_stat_monitor.pgsm_normalized_query | Selects whether save query in normalized format. pg_stat_monitor.pgsm_max_buckets | Sets the maximum number of buckets. pg_stat_monitor.pgsm_bucket_time | Sets the time in seconds per bucket. pg_stat_monitor.pgsm_histogram_min | Sets the time in millisecond. pg_stat_monitor.pgsm_histogram_max | Sets the time in millisecond. pg_stat_monitor.pgsm_histogram_buckets | Sets the maximum number of histogram buckets pg_stat_monitor.pgsm_query_shared_buffer | Sets the maximum size of shared memory in (MB) used for query tracked by pg_stat_monitor. pg_stat_monitor.pgsm_overflow_target | Sets the overflow target for pg_stat_monitor pg_stat_monitor.pgsm_enable_query_plan | Enable/Disable query plan monitoring pg_stat_monitor.pgsm_track_planning | Selects whether planning statistics are tracked. You can change a parameter by setting a new value in the configuration file. Some parameters require server restart to apply a new value. For others, configuration reload is enough. Refer to the configuration section of the pg_stat_monitor documentation for the parameters\u2019 description, how you can change their values and if the server restart is required to apply them. As an example, let\u2019s set the bucket lifetime from default 60 seconds to 100 seconds. Use the ALTER SYSTEM command: $ ALTER SYSTEM set pg_stat_monitor.pgsm_bucket_time = 100; Restart the server to apply the change: On Debian and Ubuntu $ sudo systemctl restart restart postgresql.service On Red Hat Enterprise Linux and CentOS: $ sudo systemctl restart postgresql-14 Verify the updated parameter: $ SELECT name, value FROM pg_stat_monitor_settings WHERE name = 'pg_stat_monitor.pgsm_bucket_time'; name | value ----------------------------------+------- pg_stat_monitor.pgsm_bucket_time | 100 Seealso pg_stat_monitor Documentation Percona Blog: pg_stat_monitor: A New Way Of Looking At PostgreSQL Metrics Improve PostgreSQL Query Performance Insights with pg_stat_monitor","title":"pg_stat_monitor"},{"location":"pg-stat-monitor.html#pg_stat_monitor","text":"Note This document describes the functionality of pg_stat_monitor 1.0.0.","title":"pg_stat_monitor"},{"location":"pg-stat-monitor.html#overview","text":"pg_stat_monitor is a Query Performance Monitoring tool for PostgreSQL. It collects various statistics data such as query statistics, query plan, SQL comments and other performance insights. The collected data is aggregated and presented in a single view. This allows you to view queries from performance, application and analysis perspectives. pg_stat_monitor groups statistics data and writes it in a storage unit called bucket . The data is added and stored in a bucket for the defined period \u2013 the bucket lifetime. This allows you to identify performance issues and patterns based on time. You can specify the following: The number of buckets. Together they form a bucket chain. Bucket size. This is the amount of shared memory allocated for buckets. Memory is divided equally among buckets. Bucket lifetime. When a bucket lifetime expires, pg_stat_monitor resets all statistics and writes the data in the next bucket in the chain. When the last bucket\u2019s lifetime expires, pg_stat_monitor returns to the first bucket. Important The contents of the bucket will be overwritten. In order not to lose the data, make sure to read the bucket before pg_stat_monitor starts writing new data to it.","title":"Overview"},{"location":"pg-stat-monitor.html#views","text":"pg_stat_monitor provides two views: pg_stat_monitor is the view where statistics data is presented. pg_stat_monitor_settings view shows available configuration options which you can change.","title":"Views"},{"location":"pg-stat-monitor.html#pg_stat_monitor-view","text":"The pg_stat_monitor view contains all the statistics collected and aggregated by the extension. This view contains one row for each distinct combination of metrics and whether it is a top-level statement or not (up to the maximum number of distinct statements that the module can track). For details about available metrics, refer to the pg_stat_monitor view reference . The following are the primary keys for pg_stat_monitor: bucket , userid , dbid , client_ip , application_name . A new row is created for each key in the pg_stat_monitor view. For security reasons, only superusers and members of the pg_read_all_stats role are allowed to see the SQL text and queryid of queries executed by other users. Other users can see the statistics, however, if the view has been installed in their database.","title":"pg_stat_monitor view"},{"location":"pg-stat-monitor.html#pg_stat_monitor_settings-view","text":"The pg_stat_monitor_settings view shows one row per pg_stat_monitor configuration parameter. It displays configuration parameter name, value, default value, description, minimum and maximum values, and whether a restart is required for a change in value to be effective. To learn more, see the Changing the configuration section.","title":"pg_stat_monitor_settings view"},{"location":"pg-stat-monitor.html#installation","text":"This section describes how to install pg_stat_monitor from Percona repositories. To learn about other installation methods, see the Installation section in the pg_stat_monitor documentation. Assumptions : We assume that you have installed percona-release utility and enabled the Percona Distribution for PostgreSQL repository To install pg_stat_monitor , run the following command: On Debian and Ubuntu: $ sudo apt install percona-pg-stat-monitor14 On Red Hat Enterprise Linux and CentOS: $ sudo yum install percona-pg-stat-monitor14","title":"Installation"},{"location":"pg-stat-monitor.html#setup","text":"pg_stat_monitor requires additional setup in order to use it with PostgreSQL. The setup steps are the following: Add pg_stat_monitor in the shared_preload_libraries configuration parameter. The recommended way to modify PostgreSQL configuration file is using the ALTER SYSTEM command. Connect to psql and use the following command: $ ALTER SYSTEM SET shared_preload_libraries = 'pg_stat_monitor'; The parameter value is written to the postgresql.auto.conf file which is read in addition with postgresql.conf file. Note If you\u2019ve added other values to the shared_preload_libraries parameter, list all of them separated by commas for the ALTER SYSTEM command. For example, ALTER SYSTEM SET shared_preload_libraries = \u2018foo, bar, pg_stat_monitor\u2019 . Start or restart the postgresql instance to enable pg_stat_monitor . Use the following command for restart: On Debian and Ubuntu: $ sudo systemctl restart postgresql.service On Red Hat Enterprise Linux and CentOS: $ sudo systemctl restart postgresql-14 Create the extension. Connect to psql and use the following command: $ CREATE EXTENSION pg_stat_monitor; Note By default, the extension is created against the postgres database. You need to create the extension on every database where you want to collect statistics. Tip To check the version of the extension, run the following command in the psql session: SELECT pg_stat_monitor_version () ;","title":"Setup"},{"location":"pg-stat-monitor.html#usage","text":"For example, to view the IP address of the client application that made the query, run the following command: SELECT DISTINCT userid::regrole, pg_stat_monitor.datname, substr(query,0, 50) AS query, calls, bucket, bucket_start_time, queryid, client_ip FROM pg_stat_monitor, pg_database WHERE pg_database.oid = oid; userid | datname | query | calls | client_ip ----------+----------+---------------------------------------------------+-------+----------- postgres | postgres | select bucket, bucket_start_time, query,calls fro | 1 | 127.0.0.1 postgres | postgres | SELECT c.relchecks, c.relkind, c.relhasindex, c.r | 1 | 127.0.0.1 postgres | postgres | SELECT userid, total_time, min_time, max_time, | 1 | 127.0.0.1 Find more usage examples in the pg_stat_monitor User Guide .","title":"Usage"},{"location":"pg-stat-monitor.html#changing-the-configuration","text":"Run the following query to list available configuration parameters. $ SELECT name,description FROM pg_stat_monitor_settings; Output name | description -----------------------------------------------+------------------------------------------------------------------- pg_stat_monitor.pgsm_max | Sets the maximum number of statements tracked by pg_stat_monitor. pg_stat_monitor.pgsm_query_max_len | Sets the maximum length of query. pg_stat_monitor.pgsm_enable | Enable/Disable statistics collector. pg_stat_monitor.pgsm_track_utility | Selects whether utility commands are tracked. pg_stat_monitor.pgsm_normalized_query | Selects whether save query in normalized format. pg_stat_monitor.pgsm_max_buckets | Sets the maximum number of buckets. pg_stat_monitor.pgsm_bucket_time | Sets the time in seconds per bucket. pg_stat_monitor.pgsm_histogram_min | Sets the time in millisecond. pg_stat_monitor.pgsm_histogram_max | Sets the time in millisecond. pg_stat_monitor.pgsm_histogram_buckets | Sets the maximum number of histogram buckets pg_stat_monitor.pgsm_query_shared_buffer | Sets the maximum size of shared memory in (MB) used for query tracked by pg_stat_monitor. pg_stat_monitor.pgsm_overflow_target | Sets the overflow target for pg_stat_monitor pg_stat_monitor.pgsm_enable_query_plan | Enable/Disable query plan monitoring pg_stat_monitor.pgsm_track_planning | Selects whether planning statistics are tracked. You can change a parameter by setting a new value in the configuration file. Some parameters require server restart to apply a new value. For others, configuration reload is enough. Refer to the configuration section of the pg_stat_monitor documentation for the parameters\u2019 description, how you can change their values and if the server restart is required to apply them. As an example, let\u2019s set the bucket lifetime from default 60 seconds to 100 seconds. Use the ALTER SYSTEM command: $ ALTER SYSTEM set pg_stat_monitor.pgsm_bucket_time = 100; Restart the server to apply the change: On Debian and Ubuntu $ sudo systemctl restart restart postgresql.service On Red Hat Enterprise Linux and CentOS: $ sudo systemctl restart postgresql-14 Verify the updated parameter: $ SELECT name, value FROM pg_stat_monitor_settings WHERE name = 'pg_stat_monitor.pgsm_bucket_time'; name | value ----------------------------------+------- pg_stat_monitor.pgsm_bucket_time | 100 Seealso pg_stat_monitor Documentation Percona Blog: pg_stat_monitor: A New Way Of Looking At PostgreSQL Metrics Improve PostgreSQL Query Performance Insights with pg_stat_monitor","title":"Changing the configuration"},{"location":"release-notes-v14.1.html","text":"Percona Distribution for PostgreSQL 14.1 \u00b6 Date: November 22, 2021 Installation: Installing Percona Distribution for PostgreSQL Upgrade: Upgrading Percona Distribution for PostgreSQL from 13 to 14 We are pleased to announce the launch of Percona Distribution for PostgreSQL 14.1 - a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This release of Percona Distribution for PostgreSQL is based on PostgreSQL 14.1 . Release Highlights \u00b6 PostgreSQL 14 provides an extensive set of new features and enhancements to security, performance, usability for client applications and more. Most notable of them include the following: Expired B-tree index entries can now be detected and removed between vacuum runs. This results in lesser number of page splits and reduces the index bloat. The vacuum process now deletes B-tree pages in a single cycle, without marking them as deleted during the first run. This speeds up free space cleanup. Support for subscripts in JSON is added to simplify data retrieval using a commonly recognized syntax. Stored procedures can accept OUT parameters. The libpq library now supports the pipeline mode. Previously, the client applications waited for a transaction to be completed before sending the next one. The pipeline mode allows the applications to send multiple transactions at the same time thus boosting performance. Large transactions are now streamed to subscribers in-progress, thus increasing the performance. This improvement applies to logical replication API as well. LZ4 compression is added for TOAST operations. This speeds up large data processing. SCRAM is made the default authentication mechanism. This mechanism improves security and simplifies regulatory compliance for data security. Seealso PostgreSQL 14.1 release notes Percona Blog: PostgreSQL 14 - Performance, Security, Usability, and Observability Using the Range and the New Multirange Data Type in PostgreSQL 14 Using New JSON Syntax in PostgreSQL 14 - Update The following is the list of extensions available in Percona Distribution for PostgreSQL. Extension Version Description Patroni 2.1.1 a HA (High Availability) solution for PostgreSQL Pgaudit 1.6.0 provides detailed session or object audit logging via the standard logging facility provided by PostgreSQL pgAudit set user 3.0.0 provides an additional layer of logging and control when unprivileged users must escalate themselves to superuser or object owner roles in order to perform needed maintenance tasks. pgBackRest 2.36 a backup and restore solution for PostgreSQL pgBadger 11.6 a fast PostgreSQL Log Analyzer. pgBouncer 1.16.1 lightweight connection pooler for PostgreSQL pg_repack 1.4.7 rebuilds PostgreSQL database objects pg_stat_monitor 1.0.0 - Beta2 collects and aggregates statistics for PostgreSQL and provides histogram information. PostgreSQL Common 230 PostgreSQL database-cluster manager. It provides a structure under which multiple versions of PostgreSQL may be installed and/or multiple clusters maintained at one time. wal2json 2.4 a PostgreSQL logical decoding JSON output plugin. Percona Distribution for PostgreSQL also includes the following packages: llvm 12.0.1 packages for Red Hat Enterprise Linux 8 / CentOS 8. This fixes compatibility issues with LLVM from upstream. supplemental ETCD packages which can be used for setting up Patroni clusters. These packages are available for the following operating systems: Operating System Package Version Description CentOS 7 python3-python-etcd 0.4.3 A Python client for ETCD CentOS 8 etcd 3.3.11 A consistent, distributed key-value store python3-python-etcd 0.4.3 A Python client for ETCD Debian 9 (\u2018stretch\u2019) etcd 3.3.11 A consistent, distributed key-value store python3-etcd 0.4.3 A Python client for ETCD Percona Distribution for PostgreSQL is also shipped with the libpq library. It contains \u201ca set of library functions that allow client programs to pass queries to the PostgreSQL backend server and to receive the results of these queries.\u201d","title":"Percona Distribution for PostgreSQL 14.1"},{"location":"release-notes-v14.1.html#percona-distribution-for-postgresql-141","text":"Date: November 22, 2021 Installation: Installing Percona Distribution for PostgreSQL Upgrade: Upgrading Percona Distribution for PostgreSQL from 13 to 14 We are pleased to announce the launch of Percona Distribution for PostgreSQL 14.1 - a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This release of Percona Distribution for PostgreSQL is based on PostgreSQL 14.1 .","title":"Percona Distribution for PostgreSQL 14.1"},{"location":"release-notes-v14.1.html#release-highlights","text":"PostgreSQL 14 provides an extensive set of new features and enhancements to security, performance, usability for client applications and more. Most notable of them include the following: Expired B-tree index entries can now be detected and removed between vacuum runs. This results in lesser number of page splits and reduces the index bloat. The vacuum process now deletes B-tree pages in a single cycle, without marking them as deleted during the first run. This speeds up free space cleanup. Support for subscripts in JSON is added to simplify data retrieval using a commonly recognized syntax. Stored procedures can accept OUT parameters. The libpq library now supports the pipeline mode. Previously, the client applications waited for a transaction to be completed before sending the next one. The pipeline mode allows the applications to send multiple transactions at the same time thus boosting performance. Large transactions are now streamed to subscribers in-progress, thus increasing the performance. This improvement applies to logical replication API as well. LZ4 compression is added for TOAST operations. This speeds up large data processing. SCRAM is made the default authentication mechanism. This mechanism improves security and simplifies regulatory compliance for data security. Seealso PostgreSQL 14.1 release notes Percona Blog: PostgreSQL 14 - Performance, Security, Usability, and Observability Using the Range and the New Multirange Data Type in PostgreSQL 14 Using New JSON Syntax in PostgreSQL 14 - Update The following is the list of extensions available in Percona Distribution for PostgreSQL. Extension Version Description Patroni 2.1.1 a HA (High Availability) solution for PostgreSQL Pgaudit 1.6.0 provides detailed session or object audit logging via the standard logging facility provided by PostgreSQL pgAudit set user 3.0.0 provides an additional layer of logging and control when unprivileged users must escalate themselves to superuser or object owner roles in order to perform needed maintenance tasks. pgBackRest 2.36 a backup and restore solution for PostgreSQL pgBadger 11.6 a fast PostgreSQL Log Analyzer. pgBouncer 1.16.1 lightweight connection pooler for PostgreSQL pg_repack 1.4.7 rebuilds PostgreSQL database objects pg_stat_monitor 1.0.0 - Beta2 collects and aggregates statistics for PostgreSQL and provides histogram information. PostgreSQL Common 230 PostgreSQL database-cluster manager. It provides a structure under which multiple versions of PostgreSQL may be installed and/or multiple clusters maintained at one time. wal2json 2.4 a PostgreSQL logical decoding JSON output plugin. Percona Distribution for PostgreSQL also includes the following packages: llvm 12.0.1 packages for Red Hat Enterprise Linux 8 / CentOS 8. This fixes compatibility issues with LLVM from upstream. supplemental ETCD packages which can be used for setting up Patroni clusters. These packages are available for the following operating systems: Operating System Package Version Description CentOS 7 python3-python-etcd 0.4.3 A Python client for ETCD CentOS 8 etcd 3.3.11 A consistent, distributed key-value store python3-python-etcd 0.4.3 A Python client for ETCD Debian 9 (\u2018stretch\u2019) etcd 3.3.11 A consistent, distributed key-value store python3-etcd 0.4.3 A Python client for ETCD Percona Distribution for PostgreSQL is also shipped with the libpq library. It contains \u201ca set of library functions that allow client programs to pass queries to the PostgreSQL backend server and to receive the results of these queries.\u201d","title":"Release Highlights"},{"location":"release-notes-v14.1.upd.html","text":"Percona Distribution for PostgreSQL 14.1 Update \u00b6 Date: December 2, 2021 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This update of Percona Distribution for PostgreSQL fixes the inability of a user to upgrade the postgresql-common package during the major upgrade to version 14.1 on DEB-based systems. Bugs Fixed \u00b6 DISTPG-358 : \u201cDevice or resource busy\u201d error during the major upgrade of PostgreSQL on Ubuntu","title":"Percona Distribution for PostgreSQL 14.1 Update"},{"location":"release-notes-v14.1.upd.html#percona-distribution-for-postgresql-141-update","text":"Date: December 2, 2021 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This update of Percona Distribution for PostgreSQL fixes the inability of a user to upgrade the postgresql-common package during the major upgrade to version 14.1 on DEB-based systems.","title":"Percona Distribution for PostgreSQL 14.1 Update"},{"location":"release-notes-v14.1.upd.html#bugs-fixed","text":"DISTPG-358 : \u201cDevice or resource busy\u201d error during the major upgrade of PostgreSQL on Ubuntu","title":"Bugs Fixed"},{"location":"release-notes-v14.1.upd2.html","text":"Percona Distribution for PostgreSQL 14.1 Second Update \u00b6 Date: December 7, 2021 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This update of Percona Distribution for PostgreSQL includes the latest version of pg_stat_monitor 1.0.0-RC - the statistics collection tool for PostgreSQL. We welcome your feedback on your experience with pg_stat_monitor in the public JIRA project .","title":"Percona Distribution for PostgreSQL 14.1 Second Update"},{"location":"release-notes-v14.1.upd2.html#percona-distribution-for-postgresql-141-second-update","text":"Date: December 7, 2021 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This update of Percona Distribution for PostgreSQL includes the latest version of pg_stat_monitor 1.0.0-RC - the statistics collection tool for PostgreSQL. We welcome your feedback on your experience with pg_stat_monitor in the public JIRA project .","title":"Percona Distribution for PostgreSQL 14.1 Second Update"},{"location":"release-notes-v14.2.html","text":"Percona Distribution for PostgreSQL 14.2 \u00b6 Date: March 16, 2022 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This release of Percona Distribution for PostgreSQL is based on PostgreSQL 14.2 . The following is the list of extensions available in Percona Distribution for PostgreSQL. Extension Version Description Patroni 2.1.2 a HA (High Availability) solution for PostgreSQL Pgaudit 1.6.1 provides detailed session or object audit logging via the standard logging facility provided by PostgreSQL pgAudit set user 3.0.0 provides an additional layer of logging and control when unprivileged users must escalate themselves to superuser or object owner roles in order to perform needed maintenance tasks. pgBackRest 2.37 a backup and restore solution for PostgreSQL pgBadger 11.7 a fast PostgreSQL Log Analyzer. pgBouncer 1.16.1 lightweight connection pooler for PostgreSQL pg_repack 1.4.7 rebuilds PostgreSQL database objects pg_stat_monitor 1.0.0 - rc.1 collects and aggregates statistics for PostgreSQL and provides histogram information. PostgreSQL Common 237 PostgreSQL database-cluster manager. It provides a structure under which multiple versions of PostgreSQL may be installed and/or multiple clusters maintained at one time. wal2json 2.4 a PostgreSQL logical decoding JSON output plugin. Percona Distribution for PostgreSQL also includes the following packages: llvm 12.0.1 packages for Red Hat Enterprise Linux 8 / CentOS 8. This fixes compatibility issues with LLVM from upstream. supplemental ETCD packages which can be used for setting up Patroni clusters. These packages are available for the following operating systems: Operating System Package Version Description CentOS 7 python3-python-etcd 0.4.3 A Python client for ETCD CentOS 8 etcd 3.3.11 A consistent, distributed key-value store python3-python-etcd 0.4.3 A Python client for ETCD Debian 9 (\u2018stretch\u2019) etcd 3.3.11 A consistent, distributed key-value store python3-etcd 0.4.3 A Python client for ETCD Percona Distribution for PostgreSQL is also shipped with the libpq library. It contains \u201ca set of library functions that allow client programs to pass queries to the PostgreSQL backend server and to receive the results of these queries.\u201d","title":"Percona Distribution for PostgreSQL 14.2"},{"location":"release-notes-v14.2.html#percona-distribution-for-postgresql-142","text":"Date: March 16, 2022 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This release of Percona Distribution for PostgreSQL is based on PostgreSQL 14.2 . The following is the list of extensions available in Percona Distribution for PostgreSQL. Extension Version Description Patroni 2.1.2 a HA (High Availability) solution for PostgreSQL Pgaudit 1.6.1 provides detailed session or object audit logging via the standard logging facility provided by PostgreSQL pgAudit set user 3.0.0 provides an additional layer of logging and control when unprivileged users must escalate themselves to superuser or object owner roles in order to perform needed maintenance tasks. pgBackRest 2.37 a backup and restore solution for PostgreSQL pgBadger 11.7 a fast PostgreSQL Log Analyzer. pgBouncer 1.16.1 lightweight connection pooler for PostgreSQL pg_repack 1.4.7 rebuilds PostgreSQL database objects pg_stat_monitor 1.0.0 - rc.1 collects and aggregates statistics for PostgreSQL and provides histogram information. PostgreSQL Common 237 PostgreSQL database-cluster manager. It provides a structure under which multiple versions of PostgreSQL may be installed and/or multiple clusters maintained at one time. wal2json 2.4 a PostgreSQL logical decoding JSON output plugin. Percona Distribution for PostgreSQL also includes the following packages: llvm 12.0.1 packages for Red Hat Enterprise Linux 8 / CentOS 8. This fixes compatibility issues with LLVM from upstream. supplemental ETCD packages which can be used for setting up Patroni clusters. These packages are available for the following operating systems: Operating System Package Version Description CentOS 7 python3-python-etcd 0.4.3 A Python client for ETCD CentOS 8 etcd 3.3.11 A consistent, distributed key-value store python3-python-etcd 0.4.3 A Python client for ETCD Debian 9 (\u2018stretch\u2019) etcd 3.3.11 A consistent, distributed key-value store python3-etcd 0.4.3 A Python client for ETCD Percona Distribution for PostgreSQL is also shipped with the libpq library. It contains \u201ca set of library functions that allow client programs to pass queries to the PostgreSQL backend server and to receive the results of these queries.\u201d","title":"Percona Distribution for PostgreSQL 14.2"},{"location":"release-notes-v14.2.upd.html","text":"Percona Distribution for PostgreSQL 14.2 Update \u00b6 Date: April 14, 2022 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This update of Percona Distribution for PostgreSQL includes pg_stat_monitor 1.0.0-rc.2 - the new version of the statistics collection tool for PostgreSQL. We welcome your feedback on your experience with pg_stat_monitor on our Forum and in the public JIRA project .","title":"Percona Distribution for PostgreSQL 14.2 Update"},{"location":"release-notes-v14.2.upd.html#percona-distribution-for-postgresql-142-update","text":"Date: April 14, 2022 Installation: Installing Percona Distribution for PostgreSQL Percona Distribution for PostgreSQL is a collection of tools to assist you in managing PostgreSQL. Percona Distribution for PostgreSQL installs PostgreSQL and complements it by a selection of extensions that enable solving essential practical tasks efficiently. This update of Percona Distribution for PostgreSQL includes pg_stat_monitor 1.0.0-rc.2 - the new version of the statistics collection tool for PostgreSQL. We welcome your feedback on your experience with pg_stat_monitor on our Forum and in the public JIRA project .","title":"Percona Distribution for PostgreSQL 14.2 Update"},{"location":"release-notes.html","text":"Release Notes \u00b6 Percona Distribution for PostgreSQL 14.2 Update Percona Distribution for PostgreSQL 14.2 Percona Distribution for PostgreSQL 14.1 Second Update Percona Distribution for PostgreSQL 14.1 Update Percona Distribution for PostgreSQL 14.1","title":"Release Notes"},{"location":"release-notes.html#release-notes","text":"Percona Distribution for PostgreSQL 14.2 Update Percona Distribution for PostgreSQL 14.2 Percona Distribution for PostgreSQL 14.1 Second Update Percona Distribution for PostgreSQL 14.1 Update Percona Distribution for PostgreSQL 14.1","title":"Release Notes"},{"location":"uninstalling.html","text":"Uninstalling Percona Distribution for PostgreSQL \u00b6 To uninstall Percona Distribution for PostgreSQL, remove all the installed packages and data / configuration files. NOTE : Should you need the data files later, back up your data before uninstalling Percona Distribution for PostgreSQL. On Debian and Ubuntu using apt \u00b6 To uninstall Percona Distribution for PostgreSQL on platforms that use apt package manager such as Debian or Ubuntu, complete the following steps. Run all commands as root or via sudo . Stop the Percona Distribution for PostgreSQL service. $ sudo systemctl stop postgresql.service Remove the percona-postgresql packages. $ sudo apt remove percona-postgresql-14* percona-patroni percona-pgbackrest percona-pgbadger percona-pgbouncer Remove configuration and data files. $ rm -rf /etc/postgresql/14/main On Red Hat Enterprise Linux and CentOS using yum \u00b6 To uninstall Percona Distribution for PostgreSQL on platforms that use yum package manager such as Red Hat Enterprise Linux or CentOS, complete the following steps. Run all commands as root or via sudo . Stop the Percona Distribution for PostgreSQL service. $ sudo systemctl stop postgresql-14 Remove the percona-postgresql packages $ sudo yum remove percona-postgresql14* percona-pgbadger Remove configuration and data files $ rm -rf /var/lib/pgsql/14/data","title":"Uninstall"},{"location":"uninstalling.html#uninstalling-percona-distribution-for-postgresql","text":"To uninstall Percona Distribution for PostgreSQL, remove all the installed packages and data / configuration files. NOTE : Should you need the data files later, back up your data before uninstalling Percona Distribution for PostgreSQL.","title":"Uninstalling Percona Distribution for PostgreSQL"},{"location":"uninstalling.html#on-debian-and-ubuntu-using-apt","text":"To uninstall Percona Distribution for PostgreSQL on platforms that use apt package manager such as Debian or Ubuntu, complete the following steps. Run all commands as root or via sudo . Stop the Percona Distribution for PostgreSQL service. $ sudo systemctl stop postgresql.service Remove the percona-postgresql packages. $ sudo apt remove percona-postgresql-14* percona-patroni percona-pgbackrest percona-pgbadger percona-pgbouncer Remove configuration and data files. $ rm -rf /etc/postgresql/14/main","title":"On Debian and Ubuntu using apt"},{"location":"uninstalling.html#on-red-hat-enterprise-linux-and-centos-using-yum","text":"To uninstall Percona Distribution for PostgreSQL on platforms that use yum package manager such as Red Hat Enterprise Linux or CentOS, complete the following steps. Run all commands as root or via sudo . Stop the Percona Distribution for PostgreSQL service. $ sudo systemctl stop postgresql-14 Remove the percona-postgresql packages $ sudo yum remove percona-postgresql14* percona-pgbadger Remove configuration and data files $ rm -rf /var/lib/pgsql/14/data","title":"On Red Hat Enterprise Linux and CentOS using yum"},{"location":"solutions/backup-recovery.html","text":"Backup and disaster recovery in Percona Distribution for PostgreSQL \u00b6 Overview \u00b6 A Disaster Recovery (DR) solution ensures that a system can be quickly restored to a normal operational state if something unexpected happens. When operating a database, you would back up the data as frequently as possible and have a mechanism to restore that data when needed. Disaster Recovery is often mistaken for high availability (HA), but they are two different concepts altogether: High availability ensures guaranteed service levels at all times. This solution involves configuring one or more standby systems to an active database, and the ability to switch seamlessly to that standby when the primary database becomes unavailable, for example, during a power outage or a server crash. To learn more about high-availability solutions with Percona Distribution for PostgreSQL, refer to High Availability in PostgreSQL with Patroni . Disaster Recovery protects the database instance against accidental or malicious data loss or data corruption. Disaster recovery can be achieved by using either the options provided by PostgreSQL, or external extensions. PostgreSQL disaster recovery options PostgreSQL offers multiple options for setting up database disaster recovery. pg_dump or the pg_dumpall utilities This is the basic backup approach. These tools can generate the backup of one or more PostgreSQL databases (either just the structure, or both the structure and data), then restore them through the pg_restore command. Advantages Disadvantages Easy to use 1. Backup of only one database at a time. 2. No incremental backups. 3. No point-in-time recovery since the backup is a snapshot in time. 4. Performance degradation when the database size is large. File-based backup and restore This method involves backing up the PostgreSQL data directory to a different location, and restoring it when needed. Advantages Disadvantages Consistent snapshot of the data directory or the whole data disk volume 1. Requires stopping PostgreSQL in order to copy the files. This is not practical for most production setups. 2. No backup of individual databases or tables. PostgreSQL pg_basebackup This backup tool is provided by PostgreSQL. It is used to back up data when the database instance is running. pgasebackup makes a binary copy of the database cluster files, while making sure the system is put in and out of backup mode automatically. Advantages Disadvantages 1. Supports backups when the database is running. 2. Supports point-in-time recovery 1. No incremental backups. 2. No backup of individual databases or tables. To achieve a production grade PostgreSQL disaster recovery solution, you need something that can take full or incremental database backups from a running instance, and restore from those backups at any point in time. Percona Distribution for PostgreSQL is supplied with pgBackRest : a reliable, open-source backup and recovery solution for PostgreSQL. This document focuses on the Disaster recovery solution in Percona Distribution for PostgreSQL. The Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL tutorial provides guidelines of how to set up and test this solution. pgBackRest \u00b6 pgBackRest is an easy-to-use, open-source solution that can reliably back up even the largest of PostgreSQL databases. pgBackRest supports the following backup types: full backup - a complete copy of your entire data set. differential backup - includes all data that has changed since the last full backup. While this means the backup time is slightly higher, it enables a faster restore. incremental backup - only backs up the files that have changed since the last full or differential backup, resulting in a quick backup time. To restore to a point in time, however, you will need to restore each incremental backup in the order they were taken. When it comes to restoring, pgBackRest can do a full or a delta restore. A full restore needs an empty PostgreSQL target directory. A delta restore is intelligent enough to recognize already-existing files in the PostgreSQL data directory, and update only the ones the backup contains. pgBackRest supports remote repository hosting and can even use cloud-based services like AWS S3, Google Cloud Services Cloud Storage, Azure Blob Storage for saving backup files. It supports parallel backup through multi-core processing and compression. By default, backup integrity is verified through checksums, and saved files can be encrypted for enhanced security. pgBackRest can restore a database to a specific point in time in the past. This is the case where a database is not inaccessible but perhaps contains corrupted data. Using the point-in-time recovery, a database administrator can restore the database to the last known good state. Finally, pgBackRest also supports restoring PostgreSQL databases to a different PostgreSQL instance or a separate data directory. Setup overview \u00b6 This section describes the architecture of the backup and disaster recovery solution. For the configuration steps, refer to the Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL . System architecture \u00b6 As the configuration example, we will use a three server architecture where pgBackRest resides on a dedicated remote host. The servers communicate with each other via passwordless SSH. Important Passwordless SSH may not be an ideal solution for your environment. In this case, consider using other methods, for example, TLS with client certificates . The following diagram illustrates the architecture layout: Components: \u00b6 The architecture consists of three server instances: pg-primary hosts the primary PostgreSQL server. Note that \u201cprimary\u201d here means the main database instance and does not refer to the primary node of a PostgreSQL replication cluster or a HA setup. pg-repo is the remote backup repository and hosts pgBackRest . It\u2019s important to host the backup repository on a physically separate instance, to be accessed when the target goes down. pg-secondary is the secondary PostgreSQL node. Don\u2019t confuse it with a hot standby. \u201cSecondary\u201d in this context means a PostgreSQL instance that\u2019s idle. We will restore the database backup to this instance when the primary PostgreSQL instance goes down. Note For simplicity, we use a single-node PostgreSQL instance as the primary database server. In a production scenario, you will use some form of high-availability solution to protect the primary instance. When you are using a high-availability setup, we recommend configuring pgBackRest to back up the hot standby server so the primary node is not unnecessarily loaded. Deployment \u00b6 Refer to the Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL tutorial.","title":"Backup and disaster recovery"},{"location":"solutions/backup-recovery.html#backup-and-disaster-recovery-in-percona-distribution-for-postgresql","text":"","title":"Backup and disaster recovery in Percona Distribution for PostgreSQL"},{"location":"solutions/backup-recovery.html#overview","text":"A Disaster Recovery (DR) solution ensures that a system can be quickly restored to a normal operational state if something unexpected happens. When operating a database, you would back up the data as frequently as possible and have a mechanism to restore that data when needed. Disaster Recovery is often mistaken for high availability (HA), but they are two different concepts altogether: High availability ensures guaranteed service levels at all times. This solution involves configuring one or more standby systems to an active database, and the ability to switch seamlessly to that standby when the primary database becomes unavailable, for example, during a power outage or a server crash. To learn more about high-availability solutions with Percona Distribution for PostgreSQL, refer to High Availability in PostgreSQL with Patroni . Disaster Recovery protects the database instance against accidental or malicious data loss or data corruption. Disaster recovery can be achieved by using either the options provided by PostgreSQL, or external extensions. PostgreSQL disaster recovery options PostgreSQL offers multiple options for setting up database disaster recovery. pg_dump or the pg_dumpall utilities This is the basic backup approach. These tools can generate the backup of one or more PostgreSQL databases (either just the structure, or both the structure and data), then restore them through the pg_restore command. Advantages Disadvantages Easy to use 1. Backup of only one database at a time. 2. No incremental backups. 3. No point-in-time recovery since the backup is a snapshot in time. 4. Performance degradation when the database size is large. File-based backup and restore This method involves backing up the PostgreSQL data directory to a different location, and restoring it when needed. Advantages Disadvantages Consistent snapshot of the data directory or the whole data disk volume 1. Requires stopping PostgreSQL in order to copy the files. This is not practical for most production setups. 2. No backup of individual databases or tables. PostgreSQL pg_basebackup This backup tool is provided by PostgreSQL. It is used to back up data when the database instance is running. pgasebackup makes a binary copy of the database cluster files, while making sure the system is put in and out of backup mode automatically. Advantages Disadvantages 1. Supports backups when the database is running. 2. Supports point-in-time recovery 1. No incremental backups. 2. No backup of individual databases or tables. To achieve a production grade PostgreSQL disaster recovery solution, you need something that can take full or incremental database backups from a running instance, and restore from those backups at any point in time. Percona Distribution for PostgreSQL is supplied with pgBackRest : a reliable, open-source backup and recovery solution for PostgreSQL. This document focuses on the Disaster recovery solution in Percona Distribution for PostgreSQL. The Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL tutorial provides guidelines of how to set up and test this solution.","title":"Overview"},{"location":"solutions/backup-recovery.html#pgbackrest","text":"pgBackRest is an easy-to-use, open-source solution that can reliably back up even the largest of PostgreSQL databases. pgBackRest supports the following backup types: full backup - a complete copy of your entire data set. differential backup - includes all data that has changed since the last full backup. While this means the backup time is slightly higher, it enables a faster restore. incremental backup - only backs up the files that have changed since the last full or differential backup, resulting in a quick backup time. To restore to a point in time, however, you will need to restore each incremental backup in the order they were taken. When it comes to restoring, pgBackRest can do a full or a delta restore. A full restore needs an empty PostgreSQL target directory. A delta restore is intelligent enough to recognize already-existing files in the PostgreSQL data directory, and update only the ones the backup contains. pgBackRest supports remote repository hosting and can even use cloud-based services like AWS S3, Google Cloud Services Cloud Storage, Azure Blob Storage for saving backup files. It supports parallel backup through multi-core processing and compression. By default, backup integrity is verified through checksums, and saved files can be encrypted for enhanced security. pgBackRest can restore a database to a specific point in time in the past. This is the case where a database is not inaccessible but perhaps contains corrupted data. Using the point-in-time recovery, a database administrator can restore the database to the last known good state. Finally, pgBackRest also supports restoring PostgreSQL databases to a different PostgreSQL instance or a separate data directory.","title":"pgBackRest"},{"location":"solutions/backup-recovery.html#setup-overview","text":"This section describes the architecture of the backup and disaster recovery solution. For the configuration steps, refer to the Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL .","title":"Setup overview"},{"location":"solutions/backup-recovery.html#system-architecture","text":"As the configuration example, we will use a three server architecture where pgBackRest resides on a dedicated remote host. The servers communicate with each other via passwordless SSH. Important Passwordless SSH may not be an ideal solution for your environment. In this case, consider using other methods, for example, TLS with client certificates . The following diagram illustrates the architecture layout:","title":"System architecture"},{"location":"solutions/backup-recovery.html#components","text":"The architecture consists of three server instances: pg-primary hosts the primary PostgreSQL server. Note that \u201cprimary\u201d here means the main database instance and does not refer to the primary node of a PostgreSQL replication cluster or a HA setup. pg-repo is the remote backup repository and hosts pgBackRest . It\u2019s important to host the backup repository on a physically separate instance, to be accessed when the target goes down. pg-secondary is the secondary PostgreSQL node. Don\u2019t confuse it with a hot standby. \u201cSecondary\u201d in this context means a PostgreSQL instance that\u2019s idle. We will restore the database backup to this instance when the primary PostgreSQL instance goes down. Note For simplicity, we use a single-node PostgreSQL instance as the primary database server. In a production scenario, you will use some form of high-availability solution to protect the primary instance. When you are using a high-availability setup, we recommend configuring pgBackRest to back up the hot standby server so the primary node is not unnecessarily loaded.","title":"Components:"},{"location":"solutions/backup-recovery.html#deployment","text":"Refer to the Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL tutorial.","title":"Deployment"},{"location":"solutions/dr-pgbackrest-setup.html","text":"Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL \u00b6 This document provides instructions of how to set up and test the backup and disaster recovery solution in Percona Distribution for PostgreSQL with pgBackRest . For technical overview and architecture description of this solution, refer to Backup and disaster recovery in Percona Distribution for PostgreSQL . Deployment \u00b6 As the example configuration, we will use the nodes with the following IP addresses: Node name Internal IP address pg-primary 10.104.0.3 pg-repo 10.104.0.5 pg-secondary 10.104.0.4 Set up hostnames \u00b6 In our architecture, the pgBackRest repository is located on a remote host. To allow communication among the nodes, passwordless SSH is required. To achieve this, properly setting up hostnames in the /etc/hosts files is very important. Define the hostname for every server in the /etc/hostname file. The following are the examples of how the /etc/hostname file in three nodes looks like: cat /etc/hostname pg-primary cat /etc/hostname pg-repo cat /etc/hostname pg-secondary For the nodes to communicate seamlessly across the network, resolve their hostnames to their IP addresses in the /etc/hosts file. (Alternatively, you can make appropriate entries in your internal DNS servers) The /etc/hosts file for the pg-primary node looks like this: ``` 127.0.1.1 pg-primary pg-primary 127.0.0.1 localhost 10.104.0.5 pg-repo ``` The /etc/hosts file in the pg-repo node looks like this: ``` 127.0.1.1 pg-repo pg-repo 127.0.0.1 localhost 10.104.0.3 pg-primary 10.104.0.4 pg-secondary ``` The /etc/hosts file in the pg-secondary node is shown below: ``` 127.0.1.1 pg-secondary pg-secondary 127.0.0.1 localhost 10.104.0.3 pg-primary 10.104.0.5 pg-repo ``` Set up passwordless SSH \u00b6 Before setting up passwordless SSH, ensure that the postgres user in all three instances has a password. To set or change the password, run the following command as a root user : passwd postgres Type the new password and confirm it. After setting up the password, edit the /etc/ssh/sshd_config file and ensure the PasswordAuthentication variable is set as yes . PasswordAuthentication yes In the pg-repo node, restart the sshd service. Without the restart, the SSH server will not allow you to connect to it using a password while adding the keys. sudo service sshd restart In the pg-primary node, generate an SSH key pair and add the public key to the pg-repo node. Important Run the commands as the postgres user. Generate SSH keys: ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: ... Copy the public key to the pg-repo node: ssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pg-repo /usr/bin/ssh-copy-id: INFO: Source of key ( s ) to be installed: \"/root/.ssh/id_rsa.pub\" /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key ( s ) , to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key ( s ) remain to be installed -- if you are prompted now it is to install the new keys postgres@pg-repo 's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh ' postgres@pg-repo ' \" and check to make sure that only the key(s) you wanted were added. To verify everything has worked as expected, run the following command from the pg-primary node. ssh postgres@pg-repo You should be able to connect to the pg-repo terminal without a password. Repeat the SSH connection from pg-repo to pg-primary to ensure that passwordless SSH is working. Set up bidirectional passwordless SSH between pg-repo and pg-secondary using the same method. This will allow pg-repo to recover the backups to pg-secondary . Install Percona Distribution for PostgreSQL \u00b6 Install Percona Distribution for PostgreSQL in the primary and the secondary nodes from Percona repository. Install percona-release . Enable the repository: sudo percona-release setup ppg14 Install Percona Distribution for PostgreSQL packages On Debian and Ubuntu: sudo apt install percona-postgresql-14 -y On RedHat Enterprise Linux and derivatives: sudo yum install percona-postgresql14-server Configure PostgreSQL on the primary node for continuous backup \u00b6 At this step, configure the PostgreSQL instance on the pg-primary node for continuous archiving of the WAL files. Note On Debian and Ubuntu, the path to the configuration file is /etc/postgresql/14/main/postgresql.conf . On RHEL and CentOS, the path to the configuration file is /var/lib/pgsql/14/data/ . Edit the postgresql.conf configuration file to include the following changes: archive_command = 'pgbackrest --stanza=prod_backup archive-push %p' archive_mode = on listen_addresses = '*' log_line_prefix = '' max_wal_senders = 3 wal_level = replica Once the changes are saved, restart PostgreSQL. sudo systemctl restart postgresql Install pgBackRest \u00b6 Install pgBackRest in all three instances from Percona repository. Use the following command: On Debian / Ubuntu: sudo apt-get install percona-pgbackrest On RHEL / CentOS: sudo yum install percona-pgbackrest Create the pgBackRest configuration file \u00b6 Run the following commands on all three nodes to set up the required configuration file for pgBackRest . Configure a location and permissions for the pgBackRest log rotation: sudo mkdir -p -m 770 /var/log/pgbackrest sudo chown postgres:postgres /var/log/pgbackrest Configure the location and permissions for the pgBackRest configuration file: sudo mkdir -p /etc/pgbackrest sudo mkdir -p /etc/pgbackrest/conf.d sudo touch /etc/pgbackrest/pgbackrest.conf sudo chmod 640 /etc/pgbackrest/pgbackrest.conf sudo chown postgres:postgres /etc/pgbackrest/pgbackrest.conf sudo mkdir -p /home/pgbackrest sudo chmod postgres:postgres /home/pgbackrest Update pgBackRest configuration file in the primary node \u00b6 Configure pgBackRest on the pg-primary node by setting up a stanza. A stanza is a set of configuration parameters that tells pgBackRest where to backup its files. Edit the /etc/pgbackrest/pgbackrest.conf file in the pg-primary node to include the following lines: [global] repo1-host=pg-repo repo1-host-user=postgres process-max=2 log-level-console=info log-level-file=debug [prod_backup] pg1-path=/var/lib/postgresql/14/main You can see the pg1-path attribute for the prod_backup stanza has been set to the PostgreSQL data folder. Update pgBackRest configuration file in the remote backup repository node \u00b6 Add a stanza for the pgBackRest in the pg-repo node. Edit the /etc/pgbackrest/pgbackrest.conf configuration file to include the following lines: [global] repo1-path=/home/pgbackrest/pg_backup repo1-retention-full=2 process-max=2 log-level-console=info log-level-file=debug start-fast=y stop-auto=y [prod_backup] pg1-path=/var/lib/postgresql/14/main pg1-host=pg-primary pg1-host-user=postgres pg1-port = 5432 Initialize pgBackRest stanza in the remote backup repository node \u00b6 After the configuration files are set up, it\u2019s now time to initialize the pgBackRest stanza. Run the following command in the remote backup repository node ( pg-repo ). sudo -u postgres pgbackrest --stanza = prod_backup stanza-create 2021 -11-07 11 :08:18.157 P00 INFO: stanza-create command begin 2 .36: --exec-id = 155883 -2277a3e7 --log-level-console = info --log-level-file = off --pg1-host = pg-primary --pg1-host-user = postgres --pg1-path = /var/lib/postgresql/14/main --pg1-port = 5432 --repo1-path = /home/pgbackrest/pg_backup --stanza = prod_backup 2021 -11-07 11 :08:19.453 P00 INFO: stanza-create for stanza 'prod_backup' on repo1 2021 -11-07 11 :08:19.566 P00 INFO: stanza-create command end: completed successfully ( 1412ms ) Once the stanza is created successfully, you can try out the different use cases for disaster recovery. Testing Backup and Restore with pgBackRest \u00b6 This section covers a few use cases where pgBackRest can back up and restore databases either in the same instance or a different node. Use Case 1: Create a backup with pgBackRest \u00b6 To start our testing, let\u2019s create a table in the postgres database in the pg-primary node and add some data. postgres =# CREATE TABLE CUSTOMER ( id integer , name text ); INSERT INTO CUSTOMER VALUES ( 1 , 'john' ); INSERT INTO CUSTOMER VALUES ( 2 , 'martha' ); INSERT INTO CUSTOMER VALUES ( 3 , 'mary' ); Take a full backup of the database instance. Run the following commands from the pg-repo node: pgbackrest -u postgres --stanza = prod_backup backup --type = full If you want an incremental backup, you can omit the type attribute. By default, pgBackRest always takes an incremental backup except the first backup of the cluster which is always a full backup. If you need a differential backup, use diff for the type field: pgbackrest -u postgres --stanza = prod_backup backup --type = diff Use Case 2: Restore a PostgreSQL Instance from a full backup \u00b6 For testing purposes, let\u2019s \u201cdamage\u201d the PostgreSQL instance. Run the following command in the pg-primary node to delete the main data directory. rm -rf /var/lib/postgresql/14/main/* To restore the backup, run the following commands. Stop the postgresql instance sudo systemctl stop postgresql Restore the backup: pgbackrest -u postgres --stanza = prod_backup restore Start the postgresql instance sudo systemctl start postgresql After the command executes successfully, you can access PostgreSQL from the psql command line tool and check if the table and data rows have been restored. Use Case 3: Point-In-Time Recovery \u00b6 If your target PostgreSQL instance has an already existing data directory, the full restore option will fail. You will get an error message stating there are existing data files. In this case, you can use the --delta option to restore only the corrupted files. For example, let\u2019s say one of your developers mistakenly deleted a few rows from a table. You can use pgBackRest to revert your database to a previous point in time to recover the lost rows. To test this use case, do the following: Take a timestamp when the database is stable and error-free. Run the following command from the psql prompt. postgres =# SELECT CURRENT_TIMESTAMP ; current_timestamp ------------------------------- 2021 - 11 - 07 11 : 55 : 47 . 952405 + 00 ( 1 row ) Note down the above timestamp since we will use this time in the restore command. Note that in a real life scenario, finding the correct point in time when the database was error-free may require extensive investigation. It is also important to note that all changes after the selected point will be lost after the roll back. Delete one of the customer records added before. postgres =# DELETE FROM CUSTOMER WHERE ID = 3 ; To recover the data, run a command with the noted timestamp as an argument. Run the commands below to recover the database up to that time. Stop the postgresql instance sudo systemctl stop postgresql Restore the backup pgbackrest -u postgres --stanza = prod_backup --delta \\ --type = time \"--target= 2021-11-07 11:55:47.952405+00\" \\ --target-action = promote restore Start the postgresql instance sudo systemctl start postgresql Check the database table to see if the record has been restored. postgres =# select * from customer ; id | name ----+-------- 1 | john 2 | martha 3 | mary ( 3 rows ) Use Case 4: Restoring to a Separate PostgreSQL Instance \u00b6 Sometimes a PostgreSQL server may encounter hardware issues and become completely inaccessible. In such cases, we will need to recover the database to a separate instance where pgBackRest is not initially configured. To restore the instance to a separate host, you have to first install both PostgreSQL and pgBackRest in this host. In our test setup, we already have PostgreSQL and pgBackRest installed in the third node, pg-secondary . Change the pgBackRest configuration file in the pg-secondary node as shown below. [global] repo1-host=pg-repo repo1-host-user=postgres process-max=2 log-level-console=info log-level-file=debug [prod_backup] pg1-path=/var/lib/postgresql/14/main There should be bidirectional passwordless SSH communication between pg-repo and pg-secondary . Refer to the Set up passwordless SSH section for the steps, if you haven\u2019t configured it. Stop the PostgreSQL instance sudo systemctl stop postgresql Restore the database backup from pg-repo to pg-secondary . pgbackrest - u postgres --stanza=prod_backup --delta restore 2021 - 11 - 07 13 : 34 : 08 . 897 P00 INFO : restore command begin 2 . 36 : --delta --exec-id=109728-d81c7b0b --log-level-console=info --log-level-file=debug --pg1-path=/var/lib/postgresql/14/main --process-max=2 --repo1-host=pg-repo --repo1-host-user=postgres --stanza=prod_backup 2021 - 11 - 07 13 : 34 : 09 . 784 P00 INFO : repo1 : restore backup set 20211107 - 111534 F_20211107 - 131807 I , recovery will start at 2021 - 11 - 07 13 : 18 : 07 2021 - 11 - 07 13 : 34 : 09 . 786 P00 INFO : remove invalid files / links / paths from '/var/lib/postgresql/14/main' 2021 - 11 - 07 13 : 34 : 11 . 803 P00 INFO : write updated / var / lib / postgresql / 14 / main / postgresql . auto . conf 2021 - 11 - 07 13 : 34 : 11 . 819 P00 INFO : restore global / pg_control ( performed last to ensure aborted restores cannot be started ) 2021 - 11 - 07 13 : 34 : 11 . 819 P00 INFO : restore size = 23 . 2 MB , file total = 937 2021 - 11 - 07 13 : 34 : 11 . 820 P00 INFO : restore command end : completed successfully ( 2924 ms ) After the restore completes successfully, restart PostgreSQL: sudo systemctl start postgresql Check the database contents from the local psql shell. postgres =# select * from customer ; id | name ----+-------- 1 | john 2 | martha 3 | mary ( 3 rows )","title":"Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL"},{"location":"solutions/dr-pgbackrest-setup.html#deploying-backup-and-disaster-recovery-solution-in-percona-distribution-for-postgresql","text":"This document provides instructions of how to set up and test the backup and disaster recovery solution in Percona Distribution for PostgreSQL with pgBackRest . For technical overview and architecture description of this solution, refer to Backup and disaster recovery in Percona Distribution for PostgreSQL .","title":"Deploying backup and disaster recovery solution in Percona Distribution for PostgreSQL"},{"location":"solutions/dr-pgbackrest-setup.html#deployment","text":"As the example configuration, we will use the nodes with the following IP addresses: Node name Internal IP address pg-primary 10.104.0.3 pg-repo 10.104.0.5 pg-secondary 10.104.0.4","title":"Deployment"},{"location":"solutions/dr-pgbackrest-setup.html#set-up-hostnames","text":"In our architecture, the pgBackRest repository is located on a remote host. To allow communication among the nodes, passwordless SSH is required. To achieve this, properly setting up hostnames in the /etc/hosts files is very important. Define the hostname for every server in the /etc/hostname file. The following are the examples of how the /etc/hostname file in three nodes looks like: cat /etc/hostname pg-primary cat /etc/hostname pg-repo cat /etc/hostname pg-secondary For the nodes to communicate seamlessly across the network, resolve their hostnames to their IP addresses in the /etc/hosts file. (Alternatively, you can make appropriate entries in your internal DNS servers) The /etc/hosts file for the pg-primary node looks like this: ``` 127.0.1.1 pg-primary pg-primary 127.0.0.1 localhost 10.104.0.5 pg-repo ``` The /etc/hosts file in the pg-repo node looks like this: ``` 127.0.1.1 pg-repo pg-repo 127.0.0.1 localhost 10.104.0.3 pg-primary 10.104.0.4 pg-secondary ``` The /etc/hosts file in the pg-secondary node is shown below: ``` 127.0.1.1 pg-secondary pg-secondary 127.0.0.1 localhost 10.104.0.3 pg-primary 10.104.0.5 pg-repo ```","title":"Set up hostnames"},{"location":"solutions/dr-pgbackrest-setup.html#set-up-passwordless-ssh","text":"Before setting up passwordless SSH, ensure that the postgres user in all three instances has a password. To set or change the password, run the following command as a root user : passwd postgres Type the new password and confirm it. After setting up the password, edit the /etc/ssh/sshd_config file and ensure the PasswordAuthentication variable is set as yes . PasswordAuthentication yes In the pg-repo node, restart the sshd service. Without the restart, the SSH server will not allow you to connect to it using a password while adding the keys. sudo service sshd restart In the pg-primary node, generate an SSH key pair and add the public key to the pg-repo node. Important Run the commands as the postgres user. Generate SSH keys: ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: ... Copy the public key to the pg-repo node: ssh-copy-id -i ~/.ssh/id_rsa.pub postgres@pg-repo /usr/bin/ssh-copy-id: INFO: Source of key ( s ) to be installed: \"/root/.ssh/id_rsa.pub\" /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key ( s ) , to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key ( s ) remain to be installed -- if you are prompted now it is to install the new keys postgres@pg-repo 's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh ' postgres@pg-repo ' \" and check to make sure that only the key(s) you wanted were added. To verify everything has worked as expected, run the following command from the pg-primary node. ssh postgres@pg-repo You should be able to connect to the pg-repo terminal without a password. Repeat the SSH connection from pg-repo to pg-primary to ensure that passwordless SSH is working. Set up bidirectional passwordless SSH between pg-repo and pg-secondary using the same method. This will allow pg-repo to recover the backups to pg-secondary .","title":"Set up passwordless SSH"},{"location":"solutions/dr-pgbackrest-setup.html#install-percona-distribution-for-postgresql","text":"Install Percona Distribution for PostgreSQL in the primary and the secondary nodes from Percona repository. Install percona-release . Enable the repository: sudo percona-release setup ppg14 Install Percona Distribution for PostgreSQL packages On Debian and Ubuntu: sudo apt install percona-postgresql-14 -y On RedHat Enterprise Linux and derivatives: sudo yum install percona-postgresql14-server","title":"Install Percona Distribution for PostgreSQL"},{"location":"solutions/dr-pgbackrest-setup.html#configure-postgresql-on-the-primary-node-for-continuous-backup","text":"At this step, configure the PostgreSQL instance on the pg-primary node for continuous archiving of the WAL files. Note On Debian and Ubuntu, the path to the configuration file is /etc/postgresql/14/main/postgresql.conf . On RHEL and CentOS, the path to the configuration file is /var/lib/pgsql/14/data/ . Edit the postgresql.conf configuration file to include the following changes: archive_command = 'pgbackrest --stanza=prod_backup archive-push %p' archive_mode = on listen_addresses = '*' log_line_prefix = '' max_wal_senders = 3 wal_level = replica Once the changes are saved, restart PostgreSQL. sudo systemctl restart postgresql","title":"Configure PostgreSQL on the primary node for continuous backup"},{"location":"solutions/dr-pgbackrest-setup.html#install-pgbackrest","text":"Install pgBackRest in all three instances from Percona repository. Use the following command: On Debian / Ubuntu: sudo apt-get install percona-pgbackrest On RHEL / CentOS: sudo yum install percona-pgbackrest","title":"Install pgBackRest"},{"location":"solutions/dr-pgbackrest-setup.html#create-the-pgbackrest-configuration-file","text":"Run the following commands on all three nodes to set up the required configuration file for pgBackRest . Configure a location and permissions for the pgBackRest log rotation: sudo mkdir -p -m 770 /var/log/pgbackrest sudo chown postgres:postgres /var/log/pgbackrest Configure the location and permissions for the pgBackRest configuration file: sudo mkdir -p /etc/pgbackrest sudo mkdir -p /etc/pgbackrest/conf.d sudo touch /etc/pgbackrest/pgbackrest.conf sudo chmod 640 /etc/pgbackrest/pgbackrest.conf sudo chown postgres:postgres /etc/pgbackrest/pgbackrest.conf sudo mkdir -p /home/pgbackrest sudo chmod postgres:postgres /home/pgbackrest","title":"Create the pgBackRest configuration file"},{"location":"solutions/dr-pgbackrest-setup.html#update-pgbackrest-configuration-file-in-the-primary-node","text":"Configure pgBackRest on the pg-primary node by setting up a stanza. A stanza is a set of configuration parameters that tells pgBackRest where to backup its files. Edit the /etc/pgbackrest/pgbackrest.conf file in the pg-primary node to include the following lines: [global] repo1-host=pg-repo repo1-host-user=postgres process-max=2 log-level-console=info log-level-file=debug [prod_backup] pg1-path=/var/lib/postgresql/14/main You can see the pg1-path attribute for the prod_backup stanza has been set to the PostgreSQL data folder.","title":"Update pgBackRest configuration file in the primary node"},{"location":"solutions/dr-pgbackrest-setup.html#update-pgbackrest-configuration-file-in-the-remote-backup-repository-node","text":"Add a stanza for the pgBackRest in the pg-repo node. Edit the /etc/pgbackrest/pgbackrest.conf configuration file to include the following lines: [global] repo1-path=/home/pgbackrest/pg_backup repo1-retention-full=2 process-max=2 log-level-console=info log-level-file=debug start-fast=y stop-auto=y [prod_backup] pg1-path=/var/lib/postgresql/14/main pg1-host=pg-primary pg1-host-user=postgres pg1-port = 5432","title":"Update pgBackRest configuration file in the remote backup repository node"},{"location":"solutions/dr-pgbackrest-setup.html#initialize-pgbackrest-stanza-in-the-remote-backup-repository-node","text":"After the configuration files are set up, it\u2019s now time to initialize the pgBackRest stanza. Run the following command in the remote backup repository node ( pg-repo ). sudo -u postgres pgbackrest --stanza = prod_backup stanza-create 2021 -11-07 11 :08:18.157 P00 INFO: stanza-create command begin 2 .36: --exec-id = 155883 -2277a3e7 --log-level-console = info --log-level-file = off --pg1-host = pg-primary --pg1-host-user = postgres --pg1-path = /var/lib/postgresql/14/main --pg1-port = 5432 --repo1-path = /home/pgbackrest/pg_backup --stanza = prod_backup 2021 -11-07 11 :08:19.453 P00 INFO: stanza-create for stanza 'prod_backup' on repo1 2021 -11-07 11 :08:19.566 P00 INFO: stanza-create command end: completed successfully ( 1412ms ) Once the stanza is created successfully, you can try out the different use cases for disaster recovery.","title":"Initialize pgBackRest stanza in the remote backup repository node"},{"location":"solutions/dr-pgbackrest-setup.html#testing-backup-and-restore-with-pgbackrest","text":"This section covers a few use cases where pgBackRest can back up and restore databases either in the same instance or a different node.","title":"Testing Backup and Restore with pgBackRest"},{"location":"solutions/dr-pgbackrest-setup.html#use-case-1-create-a-backup-with-pgbackrest","text":"To start our testing, let\u2019s create a table in the postgres database in the pg-primary node and add some data. postgres =# CREATE TABLE CUSTOMER ( id integer , name text ); INSERT INTO CUSTOMER VALUES ( 1 , 'john' ); INSERT INTO CUSTOMER VALUES ( 2 , 'martha' ); INSERT INTO CUSTOMER VALUES ( 3 , 'mary' ); Take a full backup of the database instance. Run the following commands from the pg-repo node: pgbackrest -u postgres --stanza = prod_backup backup --type = full If you want an incremental backup, you can omit the type attribute. By default, pgBackRest always takes an incremental backup except the first backup of the cluster which is always a full backup. If you need a differential backup, use diff for the type field: pgbackrest -u postgres --stanza = prod_backup backup --type = diff","title":"Use Case 1: Create a backup with pgBackRest"},{"location":"solutions/dr-pgbackrest-setup.html#use-case-2-restore-a-postgresql-instance-from-a-full-backup","text":"For testing purposes, let\u2019s \u201cdamage\u201d the PostgreSQL instance. Run the following command in the pg-primary node to delete the main data directory. rm -rf /var/lib/postgresql/14/main/* To restore the backup, run the following commands. Stop the postgresql instance sudo systemctl stop postgresql Restore the backup: pgbackrest -u postgres --stanza = prod_backup restore Start the postgresql instance sudo systemctl start postgresql After the command executes successfully, you can access PostgreSQL from the psql command line tool and check if the table and data rows have been restored.","title":"Use Case 2: Restore a PostgreSQL Instance from a full backup"},{"location":"solutions/dr-pgbackrest-setup.html#use-case-3-point-in-time-recovery","text":"If your target PostgreSQL instance has an already existing data directory, the full restore option will fail. You will get an error message stating there are existing data files. In this case, you can use the --delta option to restore only the corrupted files. For example, let\u2019s say one of your developers mistakenly deleted a few rows from a table. You can use pgBackRest to revert your database to a previous point in time to recover the lost rows. To test this use case, do the following: Take a timestamp when the database is stable and error-free. Run the following command from the psql prompt. postgres =# SELECT CURRENT_TIMESTAMP ; current_timestamp ------------------------------- 2021 - 11 - 07 11 : 55 : 47 . 952405 + 00 ( 1 row ) Note down the above timestamp since we will use this time in the restore command. Note that in a real life scenario, finding the correct point in time when the database was error-free may require extensive investigation. It is also important to note that all changes after the selected point will be lost after the roll back. Delete one of the customer records added before. postgres =# DELETE FROM CUSTOMER WHERE ID = 3 ; To recover the data, run a command with the noted timestamp as an argument. Run the commands below to recover the database up to that time. Stop the postgresql instance sudo systemctl stop postgresql Restore the backup pgbackrest -u postgres --stanza = prod_backup --delta \\ --type = time \"--target= 2021-11-07 11:55:47.952405+00\" \\ --target-action = promote restore Start the postgresql instance sudo systemctl start postgresql Check the database table to see if the record has been restored. postgres =# select * from customer ; id | name ----+-------- 1 | john 2 | martha 3 | mary ( 3 rows )","title":"Use Case 3: Point-In-Time Recovery"},{"location":"solutions/dr-pgbackrest-setup.html#use-case-4-restoring-to-a-separate-postgresql-instance","text":"Sometimes a PostgreSQL server may encounter hardware issues and become completely inaccessible. In such cases, we will need to recover the database to a separate instance where pgBackRest is not initially configured. To restore the instance to a separate host, you have to first install both PostgreSQL and pgBackRest in this host. In our test setup, we already have PostgreSQL and pgBackRest installed in the third node, pg-secondary . Change the pgBackRest configuration file in the pg-secondary node as shown below. [global] repo1-host=pg-repo repo1-host-user=postgres process-max=2 log-level-console=info log-level-file=debug [prod_backup] pg1-path=/var/lib/postgresql/14/main There should be bidirectional passwordless SSH communication between pg-repo and pg-secondary . Refer to the Set up passwordless SSH section for the steps, if you haven\u2019t configured it. Stop the PostgreSQL instance sudo systemctl stop postgresql Restore the database backup from pg-repo to pg-secondary . pgbackrest - u postgres --stanza=prod_backup --delta restore 2021 - 11 - 07 13 : 34 : 08 . 897 P00 INFO : restore command begin 2 . 36 : --delta --exec-id=109728-d81c7b0b --log-level-console=info --log-level-file=debug --pg1-path=/var/lib/postgresql/14/main --process-max=2 --repo1-host=pg-repo --repo1-host-user=postgres --stanza=prod_backup 2021 - 11 - 07 13 : 34 : 09 . 784 P00 INFO : repo1 : restore backup set 20211107 - 111534 F_20211107 - 131807 I , recovery will start at 2021 - 11 - 07 13 : 18 : 07 2021 - 11 - 07 13 : 34 : 09 . 786 P00 INFO : remove invalid files / links / paths from '/var/lib/postgresql/14/main' 2021 - 11 - 07 13 : 34 : 11 . 803 P00 INFO : write updated / var / lib / postgresql / 14 / main / postgresql . auto . conf 2021 - 11 - 07 13 : 34 : 11 . 819 P00 INFO : restore global / pg_control ( performed last to ensure aborted restores cannot be started ) 2021 - 11 - 07 13 : 34 : 11 . 819 P00 INFO : restore size = 23 . 2 MB , file total = 937 2021 - 11 - 07 13 : 34 : 11 . 820 P00 INFO : restore command end : completed successfully ( 2924 ms ) After the restore completes successfully, restart PostgreSQL: sudo systemctl start postgresql Check the database contents from the local psql shell. postgres =# select * from customer ; id | name ----+-------- 1 | john 2 | martha 3 | mary ( 3 rows )","title":"Use Case 4: Restoring to a Separate PostgreSQL Instance"},{"location":"solutions/ha-setup-apt.html","text":"Deploying PostgreSQL for high availability with Patroni on Debian or Ubuntu \u00b6 This guide provides instructions on how to set up a highly available PostgreSQL cluster with Patroni on Debian or Ubuntu. Preconditions \u00b6 For this setup, we will use the nodes running on Ubuntu 20.04 as the base operating system and having the following IP addresses: Node name Public IP address Internal IP address node1 157.230.42.174 10.104.0.7 node2 68.183.177.183 10.104.0.2 node3 165.22.62.167 10.104.0.8 HAProxy-demo 134.209.111.138 10.104.0.6 Note In a production (or even non-production) setup, the PostgreSQL nodes will be within a private subnet without any public connectivity to the Internet, and the HAProxy will be in a different subnet that allows client traffic coming only from a selected IP range. To keep things simple, we have implemented this architecture in a DigitalOcean VPS environment, and each node can access the other by its internal, private IP. Setting up hostnames in the /etc/hosts file \u00b6 To make the nodes aware of each other and allow their seamless communication, resolve their hostnames to their public IP addresses. Modify the /etc/hosts file of each node as follows: node 1 node 2 node 3 127.0.0.1 localhost node1 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 127.0.0.1 localhost node2 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 127.0.0.1 localhost node3 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 The /etc/hosts file of the HAProxy-demo node looks like the following: 127.0.1.1 HAProxy-demo HAProxy-demo 127.0.0.1 localhost 10.104.0.6 HAProxy-demo 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 Install Percona Distribution for PostgreSQL \u00b6 Follow the installation instructions to install Percona Distribution for PostgreSQL on node1 , node2 and node3 . Remove the data directory. Patroni requires a clean environment to initialize a new cluster. Use the following commands to stop the PostgreSQL service and then remove the data directory: $ sudo systemctl stop postgresql $ sudo rm -rf /var/lib/postgresql/14/main Configure ETCD distributed store \u00b6 The distributed configuration store helps establish a consensus among nodes during a failover and will manage the configuration for the three PostgreSQL instances. Although Patroni can work with other distributed consensus stores (i.e., Zookeeper, Consul, etc.), the most commonly used one is etcd . The etcd cluster is first started in one node and then the subsequent nodes are added to the first node using the add command. The configuration is stored in the /etc/default/etcd file. Install etcd on every PostgreSQL node using the following command: $ sudo apt install etcd Modify the /etc/default/etcd configuration file on each node. On node1 , add the IP address of node1 to the ETCD_INITIAL_CLUSTER parameter. The configuration file looks as follows: ETCD_NAME=node1 ETCD_INITIAL_CLUSTER=\"node1=http://10.104.0.7:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"devops_token\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.7:2380\" ETCD_DATA_DIR=\"/var/lib/etcd/postgresql\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.7:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.7:2379,http://localhost:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.7:2379\" \u2026 On node2 , add the IP addresses of both node1 and node2 to the ETCD_INITIAL_CLUSTER parameter: ETCD_NAME=node2 ETCD_INITIAL_CLUSTER=\"node1=http://10.104.0.7:2380,node2=http://10.104.0.2:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"devops_token\" ETCD_INITIAL_CLUSTER_STATE=\"existing\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.2:2380\" ETCD_DATA_DIR=\"/var/lib/etcd/postgresql\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.2:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.2:2379,http://localhost:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.2:2379\" \u2026 On node3 , the ETCD_INITIAL_CLUSTER parameter includes the IP addresses of all three nodes: ETCD_NAME=node3 ETCD_INITIAL_CLUSTER=\"node1=http://10.104.0.7:2380,node2=http://10.104.0.2:2380,node3=http://10.104.0.8:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"devops_token\" ETCD_INITIAL_CLUSTER_STATE=\"existing\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.8:2380\" ETCD_DATA_DIR=\"/var/lib/etcd/postgresql\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.8:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.8:2379,http://localhost:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.8:2379\" \u2026 On node1 , add node2 and node3 to the cluster using the add command: $ sudo etcdctl member add node2 http://10.104.0.2:2380 $ sudo etcdctl member add node3 http://10.104.0.8:2380 Restart the etcd service on node2 and node3 : $ sudo systemctl restart etcd Check the etcd cluster members. $ sudo etcdctl member list The output resembles the following: 21d50d7f768f153a: name=node1 peerURLs=http://10.104.0.7:2380 clientURLs=http://10.104.0.7:2379 isLeader=true af4661d829a39112: name=node2 peerURLs=http://10.104.0.2:2380 clientURLs=http://10.104.0.2:2379 isLeader=false e3f3c0c1d12e9097: name=node3 peerURLs=http://10.104.0.8:2380 clientURLs=http://10.104.0.8:2379 isLeader=false Set up the watchdog service \u00b6 The Linux kernel uses the utility called a watchdog to protect against an unresponsive system. The watchdog monitors a system for unrecoverable application errors, depleted system resources, etc., and initiates a reboot to safely return the system to a working state. The watchdog functionality is useful for servers that are intended to run without human intervention for a long time. Instead of users finding a hung server, the watchdog functionality can help maintain the service. In this example, we will configure Softdog - a standard software implementation for watchdog that is shipped with Ubuntu 20.04. Complete the following steps on all three PostgreSQL nodes to load and configure Softdog. Load Softdog: $ sudo sh -c 'echo \"softdog\" >> /etc/modules' Patroni will be interacting with the watchdog service. Since Patroni is run by the postgres user, this user must have access to Softdog. To make this happen, change the ownership of the watchdog.rules file to the postgres user: $ sudo sh -c 'echo \"KERNEL==\\\"watchdog\\\", OWNER=\\\"postgres\\\", GROUP=\\\"postgres\\\"\" >> /etc/udev/rules.d/61-watchdog.rules' Remove Softdog from the blacklist. Find out the files where Softdog is blacklisted: $ grep blacklist /lib/modprobe.d/* /etc/modprobe.d/* | grep softdog In our case, modprobe is blacklisting the Softdog: /lib/modprobe.d/blacklist_linux_5.4.0-73-generic.conf:blacklist softdog Remove the blacklist softdog line from the /lib/modprobe.d/blacklist_linux_5.4.0-73-generic.conf file. Restart the service $ sudo modprobe softdog Verify the modprobe is working correctly by running the lsmod command: $ sudo lsmod | grep softdog The output will show a process identifier if it\u2019s running. softdog 16384 0 Check that the Softdog files under the /dev/ folder are owned by the postgres user: $ ls -l /dev/watchdog* crw-rw---- 1 postgres postgres 10, 130 Sep 11 12:53 /dev/watchdog crw------- 1 root root 245, 0 Sep 11 12:53 /dev/watchdog0 Tip If the ownership has not been changed for any reason, run the following command to manually change it: $ sudo chown postgres:postgres /dev/watchdog* Configure Patroni \u00b6 Install Patroni on every PostgreSQL node: $ sudo apt install percona-patroni Create the patroni.yml configuration file under the /etc/patroni directory. The file holds the default configuration values for a PostgreSQL cluster and will reflect the current cluster setup. Add the following configuration for node1 : scope : stampede1 name : node1 restapi : listen : 0.0.0.0:8008 connect_address : node1:8008 etcd : host : node1:2379 bootstrap : # this section will be written into Etcd:/<namespace>/<scope>/config after initializing new cluster dcs : ttl : 30 loop_wait : 10 retry_timeout : 10 maximum_lag_on_failover : 1048576 # primary_start_timeout: 300 # synchronous_mode: false postgresql : use_pg_rewind : true use_slots : true parameters : wal_level : replica hot_standby : \"on\" logging_collector : 'on' max_wal_senders : 5 max_replication_slots : 5 wal_log_hints : \"on\" #archive_mode: \"on\" #archive_timeout: 600 #archive_command: \"cp -f %p /home/postgres/archived/%f\" #recovery_conf: #restore_command: cp /home/postgres/archived/%f %p # some desired options for 'initdb' initdb : # Note: It needs to be a list (some options need values, others are switches) - encoding : UTF8 - data-checksums pg_hba : # Add following lines to pg_hba.conf after running 'initdb' - host all all 10.104.0.7/32 md5 - host replication replicator 127.0.0.1/32 trust - host all all 10.104.0.2/32 md5 - host all all 10.104.0.8/32 md5 - host all all 10.104.0.6/32 trust # - hostssl all all 0.0.0.0/0 md5 # Additional script to be launched after initial cluster creation (will be passed the connection URL as parameter) # post_init: /usr/local/bin/setup_cluster.sh # Some additional users users which needs to be created after initializing new cluster users : admin : password : admin options : - createrole - createdb replicator : password : password options : - replication postgresql : listen : 0.0.0.0:5432 connect_address : node1:5432 data_dir : \"/var/lib/postgresql/14/main\" bin_dir : \"/usr/lib/postgresql/14/bin\" # config_dir: pgpass : /tmp/pgpass0 authentication : replication : username : replicator password : password superuser : username : postgres password : password parameters : unix_socket_directories : '/var/run/postgresql' watchdog : mode : required # Allowed values: off, automatic, required device : /dev/watchdog safety_margin : 5 tags : nofailover : false noloadbalance : false clonefrom : false nosync : false Patroni configuration file Let\u2019s take a moment to understand the contents of the patroni.yml file. The first section provides the details of the first node ( node1 ) and its connection ports. After that, we have the etcd service and its port details. Following these, there is a bootstrap section that contains the PostgreSQL configurations and the steps to run once the database is initialized. The pg_hba.conf entries specify all the other nodes that can connect to this node and their authentication mechanism. Create the configuration files for node2 and node3 . Replace the reference to node1 with node2 and node3 , respectively. Enable and restart the patroni service on every node. Use the following commands: $ sudo systemctl enable patroni $ sudo systemctl restart patroni When Patroni starts, it initializes PostgreSQL (because the service is not currently running and the data directory is empty) following the directives in the bootstrap section of the configuration file. Troubleshooting Patroni To ensure that Patroni has started properly, check the logs using the following command: $ sudo journalctl -u patroni.service -n 100 -f The output shouldn\u2019t show any errors: \u2026 Sep 23 12:50:21 node01 systemd[1]: Started PostgreSQL high-availability manager. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,022 INFO: Selected new etcd server http://10.104.0.2:2379 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,029 INFO: No PostgreSQL configuration items changed, nothing to reload. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,168 INFO: Lock owner: None; I am node1 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,177 INFO: trying to bootstrap a new cluster Sep 23 12:50:22 node01 patroni[10140]: The files belonging to this database system will be owned by user \"postgres\". Sep 23 12:50:22 node01 patroni[10140]: This user must also own the server process. Sep 23 12:50:22 node01 patroni[10140]: The database cluster will be initialized with locale \"C.UTF-8\". Sep 23 12:50:22 node01 patroni[10140]: The default text search configuration will be set to \"english\". Sep 23 12:50:22 node01 patroni[10140]: Data page checksums are enabled. Sep 23 12:50:22 node01 patroni[10140]: creating directory /var/lib/postgresql/12/main ... ok Sep 23 12:50:22 node01 patroni[10140]: creating subdirectories ... ok Sep 23 12:50:22 node01 patroni[10140]: selecting dynamic shared memory implementation ... posix Sep 23 12:50:22 node01 patroni[10140]: selecting default max_connections ... 100 Sep 23 12:50:22 node01 patroni[10140]: selecting default shared_buffers ... 128MB Sep 23 12:50:22 node01 patroni[10140]: selecting default time zone ... Etc/UTC Sep 23 12:50:22 node01 patroni[10140]: creating configuration files ... ok Sep 23 12:50:22 node01 patroni[10140]: running bootstrap script ... ok Sep 23 12:50:23 node01 patroni[10140]: performing post-bootstrap initialization ... ok Sep 23 12:50:23 node01 patroni[10140]: syncing data to disk ... ok Sep 23 12:50:23 node01 patroni[10140]: initdb: warning: enabling \"trust\" authentication for local connections Sep 23 12:50:23 node01 patroni[10140]: You can change this by editing pg_hba.conf or using the option -A, or Sep 23 12:50:23 node01 patroni[10140]: --auth-local and --auth-host, the next time you run initdb. Sep 23 12:50:23 node01 patroni[10140]: Success. You can now start the database server using: Sep 23 12:50:23 node01 patroni[10140]: /usr/lib/postgresql/14/bin/pg_ctl -D /var/lib/postgresql/14/main -l logfile start Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] LOG: redirecting log output to logging collector process Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] HINT: Future log output will appear in directory \"log\". Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,694 INFO: postprimary pid=10156 Sep 23 12:50:23 node01 patroni[10165]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10167]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,743 INFO: establishing a new patroni connection to the postgres cluster Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,757 INFO: running post_bootstrap Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,767 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,793 INFO: initialized a new cluster Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,810 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,899 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:43 node01 patroni[10119]: 2021-09-23 12:50:43,898 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:53 node01 patroni[10119]: 2021-09-23 12:50:53,894 INFO: no action. I am (node1) the leader with the A common error is Patroni complaining about the lack of proper entries in the pg_hba.conf file. If you see such errors, you must manually add or fix the entries in that file and then restart the service. Changing the patroni.yml file and restarting the service will not have any effect here because the bootstrap section specifies the configuration to apply when PostgreSQL is first started in the node. It will not repeat the process even if the Patroni configuration file is modified and the service is restarted. If Patroni has started properly, you should be able to locally connect to a PostgreSQL node using the following command: $ sudo psql -U postgres psql ( 14 .1 ) Type \"help\" for help. postgres = # Configure HAProxy \u00b6 HAProxy node will accept client connection requests and route those to the active node of the PostgreSQL cluster. This way, a client application doesn\u2019t have to know what node in the underlying cluster is the current primary. All it needs to do is to access a single HAProxy URL and send its read/write requests there. Behind-the-scene, HAProxy routes the connection to a healthy node (as long as there is at least one healthy node available) and ensures that client application requests are never rejected. HAProxy is capable of routing write requests to the primary node and read requests - to the secondaries in a round-robin fashion so that no secondary instance is unnecessarily loaded. To make this happen, provide different ports in the HAProxy configuration file. In this deployment, writes are routed to port 5000 and reads - to port 5001. Install HAProxy on the HAProxy-demo node: $ sudo apt install haproxy The HAProxy configuration file path is: /etc/haproxy/haproxy.cfg . Specify the following configuration in this file. global maxconn 100 defaults log global mode tcp retries 2 timeout client 30m timeout connect 4s timeout server 30m timeout check 5s listen stats mode http bind *:7000 stats enable stats uri / listen primary bind *:5000 option httpchk /primary http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 listen standbys balance roundrobin bind *:5001 option httpchk /replica http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 HAProxy will use the REST APIs hosted by Patroni to check the health status of each PostgreSQL node and route the requests appropriately. Restart HAProxy: $ sudo systemctl restart haproxy Check the HAProxy logs to see if there are any errors: $ sudo journalctl -u haproxy.service -n 100 -f Testing \u00b6 See the Testing PostgreSQL cluster for the guidelines on how to test your PostgreSQL cluster for replication, failure, switchover.","title":"Deploying on Debian or Ubuntu"},{"location":"solutions/ha-setup-apt.html#deploying-postgresql-for-high-availability-with-patroni-on-debian-or-ubuntu","text":"This guide provides instructions on how to set up a highly available PostgreSQL cluster with Patroni on Debian or Ubuntu.","title":"Deploying PostgreSQL for high availability with Patroni on Debian or Ubuntu"},{"location":"solutions/ha-setup-apt.html#preconditions","text":"For this setup, we will use the nodes running on Ubuntu 20.04 as the base operating system and having the following IP addresses: Node name Public IP address Internal IP address node1 157.230.42.174 10.104.0.7 node2 68.183.177.183 10.104.0.2 node3 165.22.62.167 10.104.0.8 HAProxy-demo 134.209.111.138 10.104.0.6 Note In a production (or even non-production) setup, the PostgreSQL nodes will be within a private subnet without any public connectivity to the Internet, and the HAProxy will be in a different subnet that allows client traffic coming only from a selected IP range. To keep things simple, we have implemented this architecture in a DigitalOcean VPS environment, and each node can access the other by its internal, private IP.","title":"Preconditions"},{"location":"solutions/ha-setup-apt.html#setting-up-hostnames-in-the-etchosts-file","text":"To make the nodes aware of each other and allow their seamless communication, resolve their hostnames to their public IP addresses. Modify the /etc/hosts file of each node as follows: node 1 node 2 node 3 127.0.0.1 localhost node1 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 127.0.0.1 localhost node2 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 127.0.0.1 localhost node3 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 The /etc/hosts file of the HAProxy-demo node looks like the following: 127.0.1.1 HAProxy-demo HAProxy-demo 127.0.0.1 localhost 10.104.0.6 HAProxy-demo 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3","title":"Setting up hostnames in the /etc/hosts file"},{"location":"solutions/ha-setup-apt.html#install-percona-distribution-for-postgresql","text":"Follow the installation instructions to install Percona Distribution for PostgreSQL on node1 , node2 and node3 . Remove the data directory. Patroni requires a clean environment to initialize a new cluster. Use the following commands to stop the PostgreSQL service and then remove the data directory: $ sudo systemctl stop postgresql $ sudo rm -rf /var/lib/postgresql/14/main","title":"Install Percona Distribution for PostgreSQL"},{"location":"solutions/ha-setup-apt.html#configure-etcd-distributed-store","text":"The distributed configuration store helps establish a consensus among nodes during a failover and will manage the configuration for the three PostgreSQL instances. Although Patroni can work with other distributed consensus stores (i.e., Zookeeper, Consul, etc.), the most commonly used one is etcd . The etcd cluster is first started in one node and then the subsequent nodes are added to the first node using the add command. The configuration is stored in the /etc/default/etcd file. Install etcd on every PostgreSQL node using the following command: $ sudo apt install etcd Modify the /etc/default/etcd configuration file on each node. On node1 , add the IP address of node1 to the ETCD_INITIAL_CLUSTER parameter. The configuration file looks as follows: ETCD_NAME=node1 ETCD_INITIAL_CLUSTER=\"node1=http://10.104.0.7:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"devops_token\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.7:2380\" ETCD_DATA_DIR=\"/var/lib/etcd/postgresql\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.7:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.7:2379,http://localhost:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.7:2379\" \u2026 On node2 , add the IP addresses of both node1 and node2 to the ETCD_INITIAL_CLUSTER parameter: ETCD_NAME=node2 ETCD_INITIAL_CLUSTER=\"node1=http://10.104.0.7:2380,node2=http://10.104.0.2:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"devops_token\" ETCD_INITIAL_CLUSTER_STATE=\"existing\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.2:2380\" ETCD_DATA_DIR=\"/var/lib/etcd/postgresql\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.2:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.2:2379,http://localhost:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.2:2379\" \u2026 On node3 , the ETCD_INITIAL_CLUSTER parameter includes the IP addresses of all three nodes: ETCD_NAME=node3 ETCD_INITIAL_CLUSTER=\"node1=http://10.104.0.7:2380,node2=http://10.104.0.2:2380,node3=http://10.104.0.8:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"devops_token\" ETCD_INITIAL_CLUSTER_STATE=\"existing\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.8:2380\" ETCD_DATA_DIR=\"/var/lib/etcd/postgresql\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.8:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.8:2379,http://localhost:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.8:2379\" \u2026 On node1 , add node2 and node3 to the cluster using the add command: $ sudo etcdctl member add node2 http://10.104.0.2:2380 $ sudo etcdctl member add node3 http://10.104.0.8:2380 Restart the etcd service on node2 and node3 : $ sudo systemctl restart etcd Check the etcd cluster members. $ sudo etcdctl member list The output resembles the following: 21d50d7f768f153a: name=node1 peerURLs=http://10.104.0.7:2380 clientURLs=http://10.104.0.7:2379 isLeader=true af4661d829a39112: name=node2 peerURLs=http://10.104.0.2:2380 clientURLs=http://10.104.0.2:2379 isLeader=false e3f3c0c1d12e9097: name=node3 peerURLs=http://10.104.0.8:2380 clientURLs=http://10.104.0.8:2379 isLeader=false","title":"Configure ETCD distributed store"},{"location":"solutions/ha-setup-apt.html#set-up-the-watchdog-service","text":"The Linux kernel uses the utility called a watchdog to protect against an unresponsive system. The watchdog monitors a system for unrecoverable application errors, depleted system resources, etc., and initiates a reboot to safely return the system to a working state. The watchdog functionality is useful for servers that are intended to run without human intervention for a long time. Instead of users finding a hung server, the watchdog functionality can help maintain the service. In this example, we will configure Softdog - a standard software implementation for watchdog that is shipped with Ubuntu 20.04. Complete the following steps on all three PostgreSQL nodes to load and configure Softdog. Load Softdog: $ sudo sh -c 'echo \"softdog\" >> /etc/modules' Patroni will be interacting with the watchdog service. Since Patroni is run by the postgres user, this user must have access to Softdog. To make this happen, change the ownership of the watchdog.rules file to the postgres user: $ sudo sh -c 'echo \"KERNEL==\\\"watchdog\\\", OWNER=\\\"postgres\\\", GROUP=\\\"postgres\\\"\" >> /etc/udev/rules.d/61-watchdog.rules' Remove Softdog from the blacklist. Find out the files where Softdog is blacklisted: $ grep blacklist /lib/modprobe.d/* /etc/modprobe.d/* | grep softdog In our case, modprobe is blacklisting the Softdog: /lib/modprobe.d/blacklist_linux_5.4.0-73-generic.conf:blacklist softdog Remove the blacklist softdog line from the /lib/modprobe.d/blacklist_linux_5.4.0-73-generic.conf file. Restart the service $ sudo modprobe softdog Verify the modprobe is working correctly by running the lsmod command: $ sudo lsmod | grep softdog The output will show a process identifier if it\u2019s running. softdog 16384 0 Check that the Softdog files under the /dev/ folder are owned by the postgres user: $ ls -l /dev/watchdog* crw-rw---- 1 postgres postgres 10, 130 Sep 11 12:53 /dev/watchdog crw------- 1 root root 245, 0 Sep 11 12:53 /dev/watchdog0 Tip If the ownership has not been changed for any reason, run the following command to manually change it: $ sudo chown postgres:postgres /dev/watchdog*","title":"Set up the watchdog service"},{"location":"solutions/ha-setup-apt.html#configure-patroni","text":"Install Patroni on every PostgreSQL node: $ sudo apt install percona-patroni Create the patroni.yml configuration file under the /etc/patroni directory. The file holds the default configuration values for a PostgreSQL cluster and will reflect the current cluster setup. Add the following configuration for node1 : scope : stampede1 name : node1 restapi : listen : 0.0.0.0:8008 connect_address : node1:8008 etcd : host : node1:2379 bootstrap : # this section will be written into Etcd:/<namespace>/<scope>/config after initializing new cluster dcs : ttl : 30 loop_wait : 10 retry_timeout : 10 maximum_lag_on_failover : 1048576 # primary_start_timeout: 300 # synchronous_mode: false postgresql : use_pg_rewind : true use_slots : true parameters : wal_level : replica hot_standby : \"on\" logging_collector : 'on' max_wal_senders : 5 max_replication_slots : 5 wal_log_hints : \"on\" #archive_mode: \"on\" #archive_timeout: 600 #archive_command: \"cp -f %p /home/postgres/archived/%f\" #recovery_conf: #restore_command: cp /home/postgres/archived/%f %p # some desired options for 'initdb' initdb : # Note: It needs to be a list (some options need values, others are switches) - encoding : UTF8 - data-checksums pg_hba : # Add following lines to pg_hba.conf after running 'initdb' - host all all 10.104.0.7/32 md5 - host replication replicator 127.0.0.1/32 trust - host all all 10.104.0.2/32 md5 - host all all 10.104.0.8/32 md5 - host all all 10.104.0.6/32 trust # - hostssl all all 0.0.0.0/0 md5 # Additional script to be launched after initial cluster creation (will be passed the connection URL as parameter) # post_init: /usr/local/bin/setup_cluster.sh # Some additional users users which needs to be created after initializing new cluster users : admin : password : admin options : - createrole - createdb replicator : password : password options : - replication postgresql : listen : 0.0.0.0:5432 connect_address : node1:5432 data_dir : \"/var/lib/postgresql/14/main\" bin_dir : \"/usr/lib/postgresql/14/bin\" # config_dir: pgpass : /tmp/pgpass0 authentication : replication : username : replicator password : password superuser : username : postgres password : password parameters : unix_socket_directories : '/var/run/postgresql' watchdog : mode : required # Allowed values: off, automatic, required device : /dev/watchdog safety_margin : 5 tags : nofailover : false noloadbalance : false clonefrom : false nosync : false Patroni configuration file Let\u2019s take a moment to understand the contents of the patroni.yml file. The first section provides the details of the first node ( node1 ) and its connection ports. After that, we have the etcd service and its port details. Following these, there is a bootstrap section that contains the PostgreSQL configurations and the steps to run once the database is initialized. The pg_hba.conf entries specify all the other nodes that can connect to this node and their authentication mechanism. Create the configuration files for node2 and node3 . Replace the reference to node1 with node2 and node3 , respectively. Enable and restart the patroni service on every node. Use the following commands: $ sudo systemctl enable patroni $ sudo systemctl restart patroni When Patroni starts, it initializes PostgreSQL (because the service is not currently running and the data directory is empty) following the directives in the bootstrap section of the configuration file. Troubleshooting Patroni To ensure that Patroni has started properly, check the logs using the following command: $ sudo journalctl -u patroni.service -n 100 -f The output shouldn\u2019t show any errors: \u2026 Sep 23 12:50:21 node01 systemd[1]: Started PostgreSQL high-availability manager. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,022 INFO: Selected new etcd server http://10.104.0.2:2379 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,029 INFO: No PostgreSQL configuration items changed, nothing to reload. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,168 INFO: Lock owner: None; I am node1 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,177 INFO: trying to bootstrap a new cluster Sep 23 12:50:22 node01 patroni[10140]: The files belonging to this database system will be owned by user \"postgres\". Sep 23 12:50:22 node01 patroni[10140]: This user must also own the server process. Sep 23 12:50:22 node01 patroni[10140]: The database cluster will be initialized with locale \"C.UTF-8\". Sep 23 12:50:22 node01 patroni[10140]: The default text search configuration will be set to \"english\". Sep 23 12:50:22 node01 patroni[10140]: Data page checksums are enabled. Sep 23 12:50:22 node01 patroni[10140]: creating directory /var/lib/postgresql/12/main ... ok Sep 23 12:50:22 node01 patroni[10140]: creating subdirectories ... ok Sep 23 12:50:22 node01 patroni[10140]: selecting dynamic shared memory implementation ... posix Sep 23 12:50:22 node01 patroni[10140]: selecting default max_connections ... 100 Sep 23 12:50:22 node01 patroni[10140]: selecting default shared_buffers ... 128MB Sep 23 12:50:22 node01 patroni[10140]: selecting default time zone ... Etc/UTC Sep 23 12:50:22 node01 patroni[10140]: creating configuration files ... ok Sep 23 12:50:22 node01 patroni[10140]: running bootstrap script ... ok Sep 23 12:50:23 node01 patroni[10140]: performing post-bootstrap initialization ... ok Sep 23 12:50:23 node01 patroni[10140]: syncing data to disk ... ok Sep 23 12:50:23 node01 patroni[10140]: initdb: warning: enabling \"trust\" authentication for local connections Sep 23 12:50:23 node01 patroni[10140]: You can change this by editing pg_hba.conf or using the option -A, or Sep 23 12:50:23 node01 patroni[10140]: --auth-local and --auth-host, the next time you run initdb. Sep 23 12:50:23 node01 patroni[10140]: Success. You can now start the database server using: Sep 23 12:50:23 node01 patroni[10140]: /usr/lib/postgresql/14/bin/pg_ctl -D /var/lib/postgresql/14/main -l logfile start Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] LOG: redirecting log output to logging collector process Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] HINT: Future log output will appear in directory \"log\". Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,694 INFO: postprimary pid=10156 Sep 23 12:50:23 node01 patroni[10165]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10167]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,743 INFO: establishing a new patroni connection to the postgres cluster Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,757 INFO: running post_bootstrap Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,767 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,793 INFO: initialized a new cluster Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,810 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,899 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:43 node01 patroni[10119]: 2021-09-23 12:50:43,898 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:53 node01 patroni[10119]: 2021-09-23 12:50:53,894 INFO: no action. I am (node1) the leader with the A common error is Patroni complaining about the lack of proper entries in the pg_hba.conf file. If you see such errors, you must manually add or fix the entries in that file and then restart the service. Changing the patroni.yml file and restarting the service will not have any effect here because the bootstrap section specifies the configuration to apply when PostgreSQL is first started in the node. It will not repeat the process even if the Patroni configuration file is modified and the service is restarted. If Patroni has started properly, you should be able to locally connect to a PostgreSQL node using the following command: $ sudo psql -U postgres psql ( 14 .1 ) Type \"help\" for help. postgres = #","title":"Configure Patroni"},{"location":"solutions/ha-setup-apt.html#configure-haproxy","text":"HAProxy node will accept client connection requests and route those to the active node of the PostgreSQL cluster. This way, a client application doesn\u2019t have to know what node in the underlying cluster is the current primary. All it needs to do is to access a single HAProxy URL and send its read/write requests there. Behind-the-scene, HAProxy routes the connection to a healthy node (as long as there is at least one healthy node available) and ensures that client application requests are never rejected. HAProxy is capable of routing write requests to the primary node and read requests - to the secondaries in a round-robin fashion so that no secondary instance is unnecessarily loaded. To make this happen, provide different ports in the HAProxy configuration file. In this deployment, writes are routed to port 5000 and reads - to port 5001. Install HAProxy on the HAProxy-demo node: $ sudo apt install haproxy The HAProxy configuration file path is: /etc/haproxy/haproxy.cfg . Specify the following configuration in this file. global maxconn 100 defaults log global mode tcp retries 2 timeout client 30m timeout connect 4s timeout server 30m timeout check 5s listen stats mode http bind *:7000 stats enable stats uri / listen primary bind *:5000 option httpchk /primary http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 listen standbys balance roundrobin bind *:5001 option httpchk /replica http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 HAProxy will use the REST APIs hosted by Patroni to check the health status of each PostgreSQL node and route the requests appropriately. Restart HAProxy: $ sudo systemctl restart haproxy Check the HAProxy logs to see if there are any errors: $ sudo journalctl -u haproxy.service -n 100 -f","title":"Configure HAProxy"},{"location":"solutions/ha-setup-apt.html#testing","text":"See the Testing PostgreSQL cluster for the guidelines on how to test your PostgreSQL cluster for replication, failure, switchover.","title":"Testing"},{"location":"solutions/ha-setup-yum.html","text":"Deploying PostgreSQL for high availability with Patroni on RHEL or CentOS \u00b6 This guide provides instructions on how to set up a highly available PostgreSQL cluster with Patroni on Red Hat Enterprise Linux or CentOS. Preconditions \u00b6 For this setup, we will use the nodes running on CentOS 8 as the base operating system and having the following IP addresses: Hostname Public IP address Internal IP address node1 157.230.42.174 10.104.0.7 node2 68.183.177.183 10.104.0.2 node3 165.22.62.167 10.104.0.8 etcd 159.102.29.166 10.104.0.5 HAProxy-demo 134.209.111.138 10.104.0.6 Note In a production (or even non-production) setup, the PostgreSQL and ETCD nodes will be within a private subnet without any public connectivity to the Internet, and the HAProxy will be in a different subnet that allows client traffic coming only from a selected IP range. To keep things simple, we have implemented this architecture in a DigitalOcean VPS environment, and each node can access the other by its internal, private IP. Setting up hostnames in the /etc/hosts file \u00b6 To make the nodes aware of each other and allow their seamless communication, resolve their hostnames to their public IP addresses. Modify the /etc/hosts file of each PostgreSQL node to include the hostnames and IP addresses of the remaining nodes. The following is the /etc/hosts file for node1 : 127.0.0.1 localhost node1 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 The /etc/hosts file of the HAProxy-demo node hostnames and IP addresses of all PostgreSQL nodes: 127.0.1.1 HAProxy-demo HAProxy-demo 127.0.0.1 localhost 10.104.0.6 HAProxy-demo 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 Keep the /etc/hosts file of the etcd node unchanged. Configure ETCD distributed store \u00b6 The distributed configuration store helps establish a consensus among nodes during a failover and will manage the configuration for the three PostgreSQL instances. Although Patroni can work with other distributed consensus stores (i.e., Zookeeper, Consul, etc.), the most commonly used one is etcd . In this setup we will configure ETCD on a dedicated node. Install etcd on the ETCD node. For CentOS 8, the etcd packages are available from Percona repository: Install percona-release . Enable the repository: sudo percona-release setup ppg14 Install the etcd packages using the following command: $ sudo yum install etcd python3-python-etcd Modify the /etc/etcd/etcd.conf configuration file: [Member] ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.5:2380,http://localhost:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.5:2379,http://localhost:2379\" ETCD_NAME=\"default\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.5:2380\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.5:2379\" ETCD_INITIAL_CLUSTER=\"default=http://10.104.0.5:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\" ETCD_INITIAL_CLUSTER_STATE=\"new\" Start the etcd to apply the changes: $ sudo systemctl enable etcd $ sudo systemctl start etcd $ sudo systemctl status etcd Check the etcd cluster members. $ sudo etcdctl member list The output resembles the following: 21d50d7f768f153a: name=default peerURLs=http://10.104.0.5:2380 clientURLs=http://10.104.0.5:2379 isLeader=true Install Percona Distribution for PostgreSQL \u00b6 Install Percona Distribution for PostgreSQL on node1 , node2 and node3 from Percona repository: Install percona-release . Enable the repository: sudo percona-release setup ppg14 Install Percona Distribution for PostgreSQL packages . Important Don\u2019t initialize the cluster and start the postgresql service. The cluster initialization and setup are handled by Patroni during the bootsrapping stage. Configure Patroni \u00b6 Install Patroni on every PostgreSQL node: $ sudo yum install percona-patroni Install the Python module that enables Patroni to communicate with ETCD. sudo python3 -m pip install patroni [ etcd ] Create the directories required by Patroni Create the directory to store the configuration file and make it owned by the postgres user. sudo mkdir -p /etc/patroni/ sudo chown -R postgres:postgres /etc/patroni/ Create the data directory for Patroni. Change its ownership to the postgres user and restrict the access to it sudo mkdir /data/patroni -p sudo chown -R postgres:postgres /data/patroni sudo chmod 700 /data/patroni Create the patroni.yml configuration file. su postgres vim /etc/patroni/patroni.yml Specify the following configuration: scope : postgres namespace : /pg_cluster/ name : node1 restapi : listen : 10.104.0.7:8008 # PostgreSQL node IP address connect_address : 10.104.0.7:8008 # PostgreSQL node IP address etcd : host : 10.104.0.5:2379 # ETCD node IP address bootstrap : # this section will be written into Etcd:/<namespace>/<scope>/config after initializing new cluster dcs : ttl : 30 loop_wait : 10 retry_timeout : 10 maximum_lag_on_failover : 1048576 postgresql : use_pg_rewind : true use_slots : true parameters : wal_level : replica hot_standby : \"on\" logging_collector : 'on' max_wal_senders : 5 max_replication_slots : 5 wal_log_hints : \"on\" # some desired options for 'initdb' initdb : # Note: It needs to be a list (some options need values, others are switches) - encoding : UTF8 - data-checksums pg_hba : # Add following lines to pg_hba.conf after running 'initdb' - host replication replicator 127.0.0.1/32 md5 - host replication replicator 10.104.0.2/32 md5 - host replication replicator 10.104.0.8/32 md5 - host replication replicator 10.104.0.7/32 md5 - host all all 0.0.0.0/0 md5 # - hostssl all all 0.0.0.0/0 md5 # Some additional users users which needs to be created after initializing new cluster users : admin : password : admin options : - createrole - createdb postgresql : listen : 10.104.0.7:5432 # PostgreSQL node IP address connect_address : 10.104.0.7:5432 # PostgreSQL node IP address data_dir : /data/patroni # The datadir you created bin_dir : /usr/pgsql-14/bin pgpass : /tmp/pgpass0 authentication : replication : username : replicator password : replicator superuser : username : postgres password : postgres parameters : unix_socket_directories : '.' tags : nofailover : false noloadbalance : false clonefrom : false nosync : false Create the configuration files for node2 and node3 . Replace the node and IP address of node1 to those of node2 and node3 , respectively. Create the systemd unit file patroni.service in /etc/systemd/system . sudo vim /etc/systemd/system/patroni.service Add the following contents in the file: [Unit] Description = Runners to orchestrate a high-availability PostgreSQL After = syslog.target network.target [Service] Type = simple User = postgres Group = postgres # Start the patroni process ExecStart = /bin/patroni /etc/patroni/patroni.yml # Send HUP to reload from patroni.yml ExecReload = /bin/kill -s HUP $MAINPID # only kill the patroni process, not its children, so it will gracefully stop postgres KillMode = process # Give a reasonable amount of time for the server to start up/shut down TimeoutSec = 30 # Do not restart the service if it crashes, we want to manually inspect database on failure Restart = no [Install] WantedBy = multi-user.target Make systemd aware of the new service: $ sudo systemctl daemon-reload $ sudo systemctl enable patroni $ sudo systemctl start patroni Troubleshooting Patroni To ensure that Patroni has started properly, check the logs using the following command: ```sh $ sudo journalctl -u patroni.service -n 100 -f ``` The output shouldn't show any errors: ``` \u2026 Sep 23 12:50:21 node01 systemd[1]: Started PostgreSQL high-availability manager. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,022 INFO: Selected new etcd server http://10.104.0.2:2379 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,029 INFO: No PostgreSQL configuration items changed, nothing to reload. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,168 INFO: Lock owner: None; I am node1 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,177 INFO: trying to bootstrap a new cluster Sep 23 12:50:22 node01 patroni[10140]: The files belonging to this database system will be owned by user \"postgres\". Sep 23 12:50:22 node01 patroni[10140]: This user must also own the server process. Sep 23 12:50:22 node01 patroni[10140]: The database cluster will be initialized with locale \"C.UTF-8\". Sep 23 12:50:22 node01 patroni[10140]: The default text search configuration will be set to \"english\". Sep 23 12:50:22 node01 patroni[10140]: Data page checksums are enabled. Sep 23 12:50:22 node01 patroni[10140]: creating directory /var/lib/postgresql/12/main ... ok Sep 23 12:50:22 node01 patroni[10140]: creating subdirectories ... ok Sep 23 12:50:22 node01 patroni[10140]: selecting dynamic shared memory implementation ... posix Sep 23 12:50:22 node01 patroni[10140]: selecting default max_connections ... 100 Sep 23 12:50:22 node01 patroni[10140]: selecting default shared_buffers ... 128MB Sep 23 12:50:22 node01 patroni[10140]: selecting default time zone ... Etc/UTC Sep 23 12:50:22 node01 patroni[10140]: creating configuration files ... ok Sep 23 12:50:22 node01 patroni[10140]: running bootstrap script ... ok Sep 23 12:50:23 node01 patroni[10140]: performing post-bootstrap initialization ... ok Sep 23 12:50:23 node01 patroni[10140]: syncing data to disk ... ok Sep 23 12:50:23 node01 patroni[10140]: initdb: warning: enabling \"trust\" authentication for local connections Sep 23 12:50:23 node01 patroni[10140]: You can change this by editing pg_hba.conf or using the option -A, or Sep 23 12:50:23 node01 patroni[10140]: --auth-local and --auth-host, the next time you run initdb. Sep 23 12:50:23 node01 patroni[10140]: Success. You can now start the database server using: Sep 23 12:50:23 node01 patroni[10140]: /usr/lib/postgresql/14/bin/pg_ctl -D /var/lib/postgresql/14/main -l logfile start Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] LOG: redirecting log output to logging collector process Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] HINT: Future log output will appear in directory \"log\". Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,694 INFO: postprimary pid=10156 Sep 23 12:50:23 node01 patroni[10165]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10167]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,743 INFO: establishing a new patroni connection to the postgres cluster Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,757 INFO: running post_bootstrap Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,767 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,793 INFO: initialized a new cluster Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,810 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,899 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:43 node01 patroni[10119]: 2021-09-23 12:50:43,898 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:53 node01 patroni[10119]: 2021-09-23 12:50:53,894 INFO: no action. I am (node1) the leader with the ``` A common error is Patroni complaining about the lack of proper entries in the pg_hba.conf file. If you see such errors, you must manually add or fix the entries in that file and then restart the service. Changing the patroni.yml file and restarting the service will not have any effect here because the bootstrap section specifies the configuration to apply when PostgreSQL is first started in the node. It will not repeat the process even if the Patroni configuration file is modified and the service is restarted. If Patroni has started properly, you should be able to locally connect to a PostgreSQL node using the following command: ```sh $ sudo psql -U postgres psql (14.1) Type \"help\" for help. postgres=# ``` Configure, enable and start Patroni on the remaining nodes. When all nodes are up and running, you can check the cluster status using the following command: $ sudo patronictl -c /etc/patroni/patroni.yml list + Cluster: postgres ( 7011110722654005156 ) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Leader | running | 1 | | | node2 | node2 | Replica | running | 1 | 0 | | node3 | node3 | Replica | running | 1 | 0 | +--------+-------+---------+---------+----+-----------+ Configure HAProxy \u00b6 HAProxy node will accept client connection requests and route those to the active node of the PostgreSQL cluster. This way, a client application doesn\u2019t have to know what node in the underlying cluster is the current primary. All it needs to do is to access a single HAProxy URL and send its read/write requests there. Behind-the-scene, HAProxy routes the connection to a healthy node (as long as there is at least one healthy node available) and ensures that client application requests are never rejected. HAProxy is capable of routing write requests to the primary node and read requests - to the secondaries in a round-robin fashion so that no secondary instance is unnecessarily loaded. To make this happen, provide different ports in the HAProxy configuration file. In this deployment, writes are routed to port 5000 and reads - to port 5001. Install HAProxy on the HAProxy-demo node: $ sudo yum install haproxy The HAProxy configuration file path is: /etc/haproxy/haproxy.cfg . Specify the following configuration in this file. global maxconn 100 defaults log global mode tcp retries 2 timeout client 30m timeout connect 4s timeout server 30m timeout check 5s listen stats mode http bind *:7000 stats enable stats uri / listen primary bind *:5000 option httpchk /primary http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 listen standbys balance roundrobin bind *:5001 option httpchk /replica http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 HAProxy will use the REST APIs hosted by Patroni to check the health status of each PostgreSQL node and route the requests appropriately. Enable a SELinux boolean to allow HAProxy to bind to non standard ports: sudo setsebool -P haproxy_connect_any on Restart HAProxy: $ sudo systemctl restart haproxy Check the HAProxy logs to see if there are any errors: $ sudo journalctl -u haproxy.service -n 100 -f","title":"Deploying on RHEL or CentOS"},{"location":"solutions/ha-setup-yum.html#deploying-postgresql-for-high-availability-with-patroni-on-rhel-or-centos","text":"This guide provides instructions on how to set up a highly available PostgreSQL cluster with Patroni on Red Hat Enterprise Linux or CentOS.","title":"Deploying PostgreSQL for high availability with Patroni on RHEL or CentOS"},{"location":"solutions/ha-setup-yum.html#preconditions","text":"For this setup, we will use the nodes running on CentOS 8 as the base operating system and having the following IP addresses: Hostname Public IP address Internal IP address node1 157.230.42.174 10.104.0.7 node2 68.183.177.183 10.104.0.2 node3 165.22.62.167 10.104.0.8 etcd 159.102.29.166 10.104.0.5 HAProxy-demo 134.209.111.138 10.104.0.6 Note In a production (or even non-production) setup, the PostgreSQL and ETCD nodes will be within a private subnet without any public connectivity to the Internet, and the HAProxy will be in a different subnet that allows client traffic coming only from a selected IP range. To keep things simple, we have implemented this architecture in a DigitalOcean VPS environment, and each node can access the other by its internal, private IP.","title":"Preconditions"},{"location":"solutions/ha-setup-yum.html#setting-up-hostnames-in-the-etchosts-file","text":"To make the nodes aware of each other and allow their seamless communication, resolve their hostnames to their public IP addresses. Modify the /etc/hosts file of each PostgreSQL node to include the hostnames and IP addresses of the remaining nodes. The following is the /etc/hosts file for node1 : 127.0.0.1 localhost node1 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 The /etc/hosts file of the HAProxy-demo node hostnames and IP addresses of all PostgreSQL nodes: 127.0.1.1 HAProxy-demo HAProxy-demo 127.0.0.1 localhost 10.104.0.6 HAProxy-demo 10.104.0.7 node1 10.104.0.2 node2 10.104.0.8 node3 Keep the /etc/hosts file of the etcd node unchanged.","title":"Setting up hostnames in the /etc/hosts file"},{"location":"solutions/ha-setup-yum.html#configure-etcd-distributed-store","text":"The distributed configuration store helps establish a consensus among nodes during a failover and will manage the configuration for the three PostgreSQL instances. Although Patroni can work with other distributed consensus stores (i.e., Zookeeper, Consul, etc.), the most commonly used one is etcd . In this setup we will configure ETCD on a dedicated node. Install etcd on the ETCD node. For CentOS 8, the etcd packages are available from Percona repository: Install percona-release . Enable the repository: sudo percona-release setup ppg14 Install the etcd packages using the following command: $ sudo yum install etcd python3-python-etcd Modify the /etc/etcd/etcd.conf configuration file: [Member] ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.104.0.5:2380,http://localhost:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.104.0.5:2379,http://localhost:2379\" ETCD_NAME=\"default\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.104.0.5:2380\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.104.0.5:2379\" ETCD_INITIAL_CLUSTER=\"default=http://10.104.0.5:2380\" ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\" ETCD_INITIAL_CLUSTER_STATE=\"new\" Start the etcd to apply the changes: $ sudo systemctl enable etcd $ sudo systemctl start etcd $ sudo systemctl status etcd Check the etcd cluster members. $ sudo etcdctl member list The output resembles the following: 21d50d7f768f153a: name=default peerURLs=http://10.104.0.5:2380 clientURLs=http://10.104.0.5:2379 isLeader=true","title":"Configure ETCD distributed store"},{"location":"solutions/ha-setup-yum.html#install-percona-distribution-for-postgresql","text":"Install Percona Distribution for PostgreSQL on node1 , node2 and node3 from Percona repository: Install percona-release . Enable the repository: sudo percona-release setup ppg14 Install Percona Distribution for PostgreSQL packages . Important Don\u2019t initialize the cluster and start the postgresql service. The cluster initialization and setup are handled by Patroni during the bootsrapping stage.","title":"Install Percona Distribution for PostgreSQL"},{"location":"solutions/ha-setup-yum.html#configure-patroni","text":"Install Patroni on every PostgreSQL node: $ sudo yum install percona-patroni Install the Python module that enables Patroni to communicate with ETCD. sudo python3 -m pip install patroni [ etcd ] Create the directories required by Patroni Create the directory to store the configuration file and make it owned by the postgres user. sudo mkdir -p /etc/patroni/ sudo chown -R postgres:postgres /etc/patroni/ Create the data directory for Patroni. Change its ownership to the postgres user and restrict the access to it sudo mkdir /data/patroni -p sudo chown -R postgres:postgres /data/patroni sudo chmod 700 /data/patroni Create the patroni.yml configuration file. su postgres vim /etc/patroni/patroni.yml Specify the following configuration: scope : postgres namespace : /pg_cluster/ name : node1 restapi : listen : 10.104.0.7:8008 # PostgreSQL node IP address connect_address : 10.104.0.7:8008 # PostgreSQL node IP address etcd : host : 10.104.0.5:2379 # ETCD node IP address bootstrap : # this section will be written into Etcd:/<namespace>/<scope>/config after initializing new cluster dcs : ttl : 30 loop_wait : 10 retry_timeout : 10 maximum_lag_on_failover : 1048576 postgresql : use_pg_rewind : true use_slots : true parameters : wal_level : replica hot_standby : \"on\" logging_collector : 'on' max_wal_senders : 5 max_replication_slots : 5 wal_log_hints : \"on\" # some desired options for 'initdb' initdb : # Note: It needs to be a list (some options need values, others are switches) - encoding : UTF8 - data-checksums pg_hba : # Add following lines to pg_hba.conf after running 'initdb' - host replication replicator 127.0.0.1/32 md5 - host replication replicator 10.104.0.2/32 md5 - host replication replicator 10.104.0.8/32 md5 - host replication replicator 10.104.0.7/32 md5 - host all all 0.0.0.0/0 md5 # - hostssl all all 0.0.0.0/0 md5 # Some additional users users which needs to be created after initializing new cluster users : admin : password : admin options : - createrole - createdb postgresql : listen : 10.104.0.7:5432 # PostgreSQL node IP address connect_address : 10.104.0.7:5432 # PostgreSQL node IP address data_dir : /data/patroni # The datadir you created bin_dir : /usr/pgsql-14/bin pgpass : /tmp/pgpass0 authentication : replication : username : replicator password : replicator superuser : username : postgres password : postgres parameters : unix_socket_directories : '.' tags : nofailover : false noloadbalance : false clonefrom : false nosync : false Create the configuration files for node2 and node3 . Replace the node and IP address of node1 to those of node2 and node3 , respectively. Create the systemd unit file patroni.service in /etc/systemd/system . sudo vim /etc/systemd/system/patroni.service Add the following contents in the file: [Unit] Description = Runners to orchestrate a high-availability PostgreSQL After = syslog.target network.target [Service] Type = simple User = postgres Group = postgres # Start the patroni process ExecStart = /bin/patroni /etc/patroni/patroni.yml # Send HUP to reload from patroni.yml ExecReload = /bin/kill -s HUP $MAINPID # only kill the patroni process, not its children, so it will gracefully stop postgres KillMode = process # Give a reasonable amount of time for the server to start up/shut down TimeoutSec = 30 # Do not restart the service if it crashes, we want to manually inspect database on failure Restart = no [Install] WantedBy = multi-user.target Make systemd aware of the new service: $ sudo systemctl daemon-reload $ sudo systemctl enable patroni $ sudo systemctl start patroni Troubleshooting Patroni To ensure that Patroni has started properly, check the logs using the following command: ```sh $ sudo journalctl -u patroni.service -n 100 -f ``` The output shouldn't show any errors: ``` \u2026 Sep 23 12:50:21 node01 systemd[1]: Started PostgreSQL high-availability manager. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,022 INFO: Selected new etcd server http://10.104.0.2:2379 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,029 INFO: No PostgreSQL configuration items changed, nothing to reload. Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,168 INFO: Lock owner: None; I am node1 Sep 23 12:50:22 node01 patroni[10119]: 2021-09-23 12:50:22,177 INFO: trying to bootstrap a new cluster Sep 23 12:50:22 node01 patroni[10140]: The files belonging to this database system will be owned by user \"postgres\". Sep 23 12:50:22 node01 patroni[10140]: This user must also own the server process. Sep 23 12:50:22 node01 patroni[10140]: The database cluster will be initialized with locale \"C.UTF-8\". Sep 23 12:50:22 node01 patroni[10140]: The default text search configuration will be set to \"english\". Sep 23 12:50:22 node01 patroni[10140]: Data page checksums are enabled. Sep 23 12:50:22 node01 patroni[10140]: creating directory /var/lib/postgresql/12/main ... ok Sep 23 12:50:22 node01 patroni[10140]: creating subdirectories ... ok Sep 23 12:50:22 node01 patroni[10140]: selecting dynamic shared memory implementation ... posix Sep 23 12:50:22 node01 patroni[10140]: selecting default max_connections ... 100 Sep 23 12:50:22 node01 patroni[10140]: selecting default shared_buffers ... 128MB Sep 23 12:50:22 node01 patroni[10140]: selecting default time zone ... Etc/UTC Sep 23 12:50:22 node01 patroni[10140]: creating configuration files ... ok Sep 23 12:50:22 node01 patroni[10140]: running bootstrap script ... ok Sep 23 12:50:23 node01 patroni[10140]: performing post-bootstrap initialization ... ok Sep 23 12:50:23 node01 patroni[10140]: syncing data to disk ... ok Sep 23 12:50:23 node01 patroni[10140]: initdb: warning: enabling \"trust\" authentication for local connections Sep 23 12:50:23 node01 patroni[10140]: You can change this by editing pg_hba.conf or using the option -A, or Sep 23 12:50:23 node01 patroni[10140]: --auth-local and --auth-host, the next time you run initdb. Sep 23 12:50:23 node01 patroni[10140]: Success. You can now start the database server using: Sep 23 12:50:23 node01 patroni[10140]: /usr/lib/postgresql/14/bin/pg_ctl -D /var/lib/postgresql/14/main -l logfile start Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] LOG: redirecting log output to logging collector process Sep 23 12:50:23 node01 patroni[10156]: 2021-09-23 12:50:23.672 UTC [10156] HINT: Future log output will appear in directory \"log\". Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,694 INFO: postprimary pid=10156 Sep 23 12:50:23 node01 patroni[10165]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10167]: localhost:5432 - accepting connections Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,743 INFO: establishing a new patroni connection to the postgres cluster Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,757 INFO: running post_bootstrap Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,767 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 12:50:23 node01 patroni[10119]: 2021-09-23 12:50:23,793 INFO: initialized a new cluster Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,810 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:33 node01 patroni[10119]: 2021-09-23 12:50:33,899 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:43 node01 patroni[10119]: 2021-09-23 12:50:43,898 INFO: no action. I am (node1) the leader with the lock Sep 23 12:50:53 node01 patroni[10119]: 2021-09-23 12:50:53,894 INFO: no action. I am (node1) the leader with the ``` A common error is Patroni complaining about the lack of proper entries in the pg_hba.conf file. If you see such errors, you must manually add or fix the entries in that file and then restart the service. Changing the patroni.yml file and restarting the service will not have any effect here because the bootstrap section specifies the configuration to apply when PostgreSQL is first started in the node. It will not repeat the process even if the Patroni configuration file is modified and the service is restarted. If Patroni has started properly, you should be able to locally connect to a PostgreSQL node using the following command: ```sh $ sudo psql -U postgres psql (14.1) Type \"help\" for help. postgres=# ``` Configure, enable and start Patroni on the remaining nodes. When all nodes are up and running, you can check the cluster status using the following command: $ sudo patronictl -c /etc/patroni/patroni.yml list + Cluster: postgres ( 7011110722654005156 ) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Leader | running | 1 | | | node2 | node2 | Replica | running | 1 | 0 | | node3 | node3 | Replica | running | 1 | 0 | +--------+-------+---------+---------+----+-----------+","title":"Configure Patroni"},{"location":"solutions/ha-setup-yum.html#configure-haproxy","text":"HAProxy node will accept client connection requests and route those to the active node of the PostgreSQL cluster. This way, a client application doesn\u2019t have to know what node in the underlying cluster is the current primary. All it needs to do is to access a single HAProxy URL and send its read/write requests there. Behind-the-scene, HAProxy routes the connection to a healthy node (as long as there is at least one healthy node available) and ensures that client application requests are never rejected. HAProxy is capable of routing write requests to the primary node and read requests - to the secondaries in a round-robin fashion so that no secondary instance is unnecessarily loaded. To make this happen, provide different ports in the HAProxy configuration file. In this deployment, writes are routed to port 5000 and reads - to port 5001. Install HAProxy on the HAProxy-demo node: $ sudo yum install haproxy The HAProxy configuration file path is: /etc/haproxy/haproxy.cfg . Specify the following configuration in this file. global maxconn 100 defaults log global mode tcp retries 2 timeout client 30m timeout connect 4s timeout server 30m timeout check 5s listen stats mode http bind *:7000 stats enable stats uri / listen primary bind *:5000 option httpchk /primary http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 listen standbys balance roundrobin bind *:5001 option httpchk /replica http-check expect status 200 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server node1 node1:5432 maxconn 100 check port 8008 server node2 node2:5432 maxconn 100 check port 8008 server node3 node3:5432 maxconn 100 check port 8008 HAProxy will use the REST APIs hosted by Patroni to check the health status of each PostgreSQL node and route the requests appropriately. Enable a SELinux boolean to allow HAProxy to bind to non standard ports: sudo setsebool -P haproxy_connect_any on Restart HAProxy: $ sudo systemctl restart haproxy Check the HAProxy logs to see if there are any errors: $ sudo journalctl -u haproxy.service -n 100 -f","title":"Configure HAProxy"},{"location":"solutions/ha-test.html","text":"Testing the Patroni PostgreSQL Cluster \u00b6 This document covers the following scenarios to test the PostgreSQL cluster: replication, connectivity, failover, and manual switchover. Testing replication \u00b6 Connect to the cluster and establish the psql session from a client machine that can connect to the HAProxy node. Use the HAProxy-demo node\u2019s public IP address: psql -U postgres -h 134.209.111.138 -p 5000 Run the following commands to create a table and insert a few rows: CREATE TABLE customer ( name text , age integer ); INSERT INTO CUSTOMER VALUES ( 'john' , 30 ); INSERT INTO CUSTOMER VALUES ( 'dawson' , 35 ); To ensure that the replication is working, we can log in to each PostgreSQL node and run a simple SQL statement against the locally running instance: $ sudo psql -U postgres -c \"SELECT * FROM CUSTOMER;\" The results on each node should be the following: name | age --------+----- john | 30 dawson | 35 (2 rows) Testing failover \u00b6 In a proper setup, client applications won\u2019t have issues connecting to the cluster, even if one or even two of the nodes go down. We will test the cluster for failover in the following scenarios: Scenario 1. Intentionally stop the PostgreSQL on the primary node and verify access to PostgreSQL. \u00b6 Run the following command on any node to check the current cluster status: $ sudo patronictl -c /etc/patroni/patroni.yml list + Cluster: stampede1 ( 7011110722654005156 ) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Leader | running | 1 | | | node2 | node2 | Replica | running | 1 | 0 | | node3 | node3 | Replica | running | 1 | 0 | +--------+-------+---------+---------+----+-----------+ node1 is the current leader. Stop Patroni in node1 to see how it changes the cluster: $ sudo systemctl stop patroni Once the service stops in node1 , check the logs in node2 and node3 using the following command: $ sudo journalctl -u patroni.service -n 100 -f Output Sep 23 14:18:13 node03 patroni[10042]: 2021-09-23 14:18:13,905 INFO: no action. I am a secondary (node3) and following a leader (node1) Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,011 INFO: Got response from node2 http://node2:8008/patroni: {\"state\": \"running\", \"postprimary_start_time\": \"2021-09-23 12:50:29.460027+00:00\", \"role\": \"replica\", \"server_version\": 130003, \"cluster_unlocked\": true, \"xlog\": {\"received_location\": 67219152, \"replayed_location\": 67219152, \"replayed_timestamp\": \"2021-09-23 13:19:50.329387+00:00\", \"paused\": false}, \"timeline\": 1, \"database_system_identifier\": \"7011110722654005156\", \"patroni\": {\"version\": \"2.1.0\", \"scope\": \"stampede1\"}} Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,031 WARNING: Request failed to node1: GET http://node1:8008/patroni (HTTPConnectionPool(host='node1', port=8008): Max retries exceeded with url: /patroni (Caused by ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))) Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,038 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,043 INFO: promoted self to leader by acquiring session lock Sep 23 14:18:20 node03 patroni[13641]: server promoting Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,049 INFO: cleared rewind state after becoming the leader Sep 23 14:18:21 node03 patroni[10042]: 2021-09-23 14:18:21,101 INFO: no action. I am (node3) the leader with the lock Sep 23 14:18:21 node03 patroni[10042]: 2021-09-23 14:18:21,117 INFO: no action. I am (node3) the leader with the lock Sep 23 14:18:31 node03 patroni[10042]: 2021-09-23 14:18:31,114 INFO: no action. I am (node3) the leader with the lock ... The logs in node3 show that the requests to node1 are failing, the watchdog is coming into action, and node3 is promoting itself as the leader: Verify that you can still access the cluster through the HAProxy instance and read data: psql -U postgres -h 10.104.0.6 -p 5000 -c \"SELECT * FROM CUSTOMER;\" name | age --------+----- john | 30 dawson | 35 (2 rows) Restart the Patroni service in node1 $ sudo systemctl start patroni Check the current cluster status: $ sudo patronictl -c /etc/patroni/patroni.yml list + Cluster: stampede1 ( 7011110722654005156 ) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Replica | running | 2 | 0 | | node2 | node2 | Replica | running | 2 | 0 | | node3 | node3 | Leader | running | 2 | | +--------+-------+---------+---------+----+-----------+ As we see, node3 remains the leader and the rest are replicas. Scenario 2. Abrupt machine shutdown or power outage \u00b6 To emulate the power outage, let\u2019s kill the service in node3 and see what happens in node1 and node2 . Identify the process ID of Patroni and then kill it with a -9 switch. $ ps aux | grep -i patroni postgres 10042 0 .1 2 .1 647132 43948 ? Ssl 12 :50 0 :09 /usr/bin/python3 /usr/bin/patroni /etc/patroni/patroni.yml $ sudo kill -9 10042 Check the logs on node2 : $ sudo journalctl -u patroni.service -n 100 -f Output Sep 23 14:40:41 node02 patroni[10577]: 2021-09-23 14:40:41,656 INFO: no action. I am a secondary (node2) and following a leader (node3) \u2026 Sep 23 14:41:01 node02 patroni[10577]: 2021-09-23 14:41:01,373 INFO: Got response from node1 http://node1:8008/patroni: {\"state\": \"running\", \"postprimary_start_time\": \"2021-09-23 14:25:30.076762+00:00\", \"role\": \"replica\", \"server_version\": 130003, \"cluster_unlocked\": true, \"xlog\": {\"received_location\": 67221352, \"replayed_location\": 67221352, \"replayed_timestamp\": null, \"paused\": false}, \"timeline\": 2, \"database_system_identifier\": \"7011110722654005156\", \"patroni\": {\"version\": \"2.1.0\", \"scope\": \"stampede1\"}} Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,364 WARNING: Request failed to node3: GET http://node3:8008/patroni (HTTPConnectionPool(host='node3', port=8008): Max retries exceeded with url: /patroni (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f57e06dffa0>, 'Connection to node3 timed out. (connect timeout=2)'))) Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,373 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,385 INFO: promoted self to leader by acquiring session lock Sep 23 14:41:03 node02 patroni[15478]: server promoting Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,397 INFO: cleared rewind state after becoming the leader Sep 23 14:41:04 node02 patroni[10577]: 2021-09-23 14:41:04,450 INFO: no action. I am (node2) the leader with the lock Sep 23 14:41:04 node02 patroni[10577]: 2021-09-23 14:41:04,475 INFO: no action. I am (node2) the leader with the lock \u2026 \u2026 node2 realizes that the leader is dead, and promotes itself as the leader. Try accessing the cluster using the HAProxy endpoint at any point in time between these operations. The cluster is still accepting connections. Manual switchover \u00b6 Typically, a manual switchover is needed for planned downtime to perform maintenance activity on the leader node. Patroni provides the switchover command to manually switch over from the leader node. Run the following command on node2 (the current leader node): $ sudo patronictl -c /etc/patroni/patroni.yml switchover Patroni asks the name of the current primary node and then the node that should take over as the switched-over primary. You can also specify the time at which the switchover should happen. To trigger the process immediately, specify the value now : primary [node2]: node2 Candidate ['node1', 'node3'] []: node1 When should the switchover take place (e.g. 2021-09-23T15:56 ) [now]: now Current cluster topology + Cluster: stampede1 (7011110722654005156) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Replica | running | 3 | 0 | | node2 | node2 | Leader | running | 3 | | | node3 | node3 | Replica | stopped | | unknown | +--------+-------+---------+---------+----+-----------+ Are you sure you want to switchover cluster stampede1, demoting current primary node2? [y/N]: y 2021-09-23 14:56:40.54009 Successfully switched over to \"node1\" + Cluster: stampede1 (7011110722654005156) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Leader | running | 3 | | | node2 | node2 | Replica | stopped | | unknown | | node3 | node3 | Replica | stopped | | unknown | +--------+-------+---------+---------+----+-----------+ Restart the Patroni service in node2 (after the \u201cplanned maintenance\u201d). The node rejoins the cluster as a secondary.","title":"Testing the Patroni PostgreSQL Cluster"},{"location":"solutions/ha-test.html#testing-the-patroni-postgresql-cluster","text":"This document covers the following scenarios to test the PostgreSQL cluster: replication, connectivity, failover, and manual switchover.","title":"Testing the Patroni PostgreSQL Cluster"},{"location":"solutions/ha-test.html#testing-replication","text":"Connect to the cluster and establish the psql session from a client machine that can connect to the HAProxy node. Use the HAProxy-demo node\u2019s public IP address: psql -U postgres -h 134.209.111.138 -p 5000 Run the following commands to create a table and insert a few rows: CREATE TABLE customer ( name text , age integer ); INSERT INTO CUSTOMER VALUES ( 'john' , 30 ); INSERT INTO CUSTOMER VALUES ( 'dawson' , 35 ); To ensure that the replication is working, we can log in to each PostgreSQL node and run a simple SQL statement against the locally running instance: $ sudo psql -U postgres -c \"SELECT * FROM CUSTOMER;\" The results on each node should be the following: name | age --------+----- john | 30 dawson | 35 (2 rows)","title":"Testing replication"},{"location":"solutions/ha-test.html#testing-failover","text":"In a proper setup, client applications won\u2019t have issues connecting to the cluster, even if one or even two of the nodes go down. We will test the cluster for failover in the following scenarios:","title":"Testing failover"},{"location":"solutions/ha-test.html#scenario-1-intentionally-stop-the-postgresql-on-the-primary-node-and-verify-access-to-postgresql","text":"Run the following command on any node to check the current cluster status: $ sudo patronictl -c /etc/patroni/patroni.yml list + Cluster: stampede1 ( 7011110722654005156 ) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Leader | running | 1 | | | node2 | node2 | Replica | running | 1 | 0 | | node3 | node3 | Replica | running | 1 | 0 | +--------+-------+---------+---------+----+-----------+ node1 is the current leader. Stop Patroni in node1 to see how it changes the cluster: $ sudo systemctl stop patroni Once the service stops in node1 , check the logs in node2 and node3 using the following command: $ sudo journalctl -u patroni.service -n 100 -f Output Sep 23 14:18:13 node03 patroni[10042]: 2021-09-23 14:18:13,905 INFO: no action. I am a secondary (node3) and following a leader (node1) Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,011 INFO: Got response from node2 http://node2:8008/patroni: {\"state\": \"running\", \"postprimary_start_time\": \"2021-09-23 12:50:29.460027+00:00\", \"role\": \"replica\", \"server_version\": 130003, \"cluster_unlocked\": true, \"xlog\": {\"received_location\": 67219152, \"replayed_location\": 67219152, \"replayed_timestamp\": \"2021-09-23 13:19:50.329387+00:00\", \"paused\": false}, \"timeline\": 1, \"database_system_identifier\": \"7011110722654005156\", \"patroni\": {\"version\": \"2.1.0\", \"scope\": \"stampede1\"}} Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,031 WARNING: Request failed to node1: GET http://node1:8008/patroni (HTTPConnectionPool(host='node1', port=8008): Max retries exceeded with url: /patroni (Caused by ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))) Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,038 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,043 INFO: promoted self to leader by acquiring session lock Sep 23 14:18:20 node03 patroni[13641]: server promoting Sep 23 14:18:20 node03 patroni[10042]: 2021-09-23 14:18:20,049 INFO: cleared rewind state after becoming the leader Sep 23 14:18:21 node03 patroni[10042]: 2021-09-23 14:18:21,101 INFO: no action. I am (node3) the leader with the lock Sep 23 14:18:21 node03 patroni[10042]: 2021-09-23 14:18:21,117 INFO: no action. I am (node3) the leader with the lock Sep 23 14:18:31 node03 patroni[10042]: 2021-09-23 14:18:31,114 INFO: no action. I am (node3) the leader with the lock ... The logs in node3 show that the requests to node1 are failing, the watchdog is coming into action, and node3 is promoting itself as the leader: Verify that you can still access the cluster through the HAProxy instance and read data: psql -U postgres -h 10.104.0.6 -p 5000 -c \"SELECT * FROM CUSTOMER;\" name | age --------+----- john | 30 dawson | 35 (2 rows) Restart the Patroni service in node1 $ sudo systemctl start patroni Check the current cluster status: $ sudo patronictl -c /etc/patroni/patroni.yml list + Cluster: stampede1 ( 7011110722654005156 ) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Replica | running | 2 | 0 | | node2 | node2 | Replica | running | 2 | 0 | | node3 | node3 | Leader | running | 2 | | +--------+-------+---------+---------+----+-----------+ As we see, node3 remains the leader and the rest are replicas.","title":"Scenario 1. Intentionally stop the PostgreSQL on the primary node and verify access to PostgreSQL."},{"location":"solutions/ha-test.html#scenario-2-abrupt-machine-shutdown-or-power-outage","text":"To emulate the power outage, let\u2019s kill the service in node3 and see what happens in node1 and node2 . Identify the process ID of Patroni and then kill it with a -9 switch. $ ps aux | grep -i patroni postgres 10042 0 .1 2 .1 647132 43948 ? Ssl 12 :50 0 :09 /usr/bin/python3 /usr/bin/patroni /etc/patroni/patroni.yml $ sudo kill -9 10042 Check the logs on node2 : $ sudo journalctl -u patroni.service -n 100 -f Output Sep 23 14:40:41 node02 patroni[10577]: 2021-09-23 14:40:41,656 INFO: no action. I am a secondary (node2) and following a leader (node3) \u2026 Sep 23 14:41:01 node02 patroni[10577]: 2021-09-23 14:41:01,373 INFO: Got response from node1 http://node1:8008/patroni: {\"state\": \"running\", \"postprimary_start_time\": \"2021-09-23 14:25:30.076762+00:00\", \"role\": \"replica\", \"server_version\": 130003, \"cluster_unlocked\": true, \"xlog\": {\"received_location\": 67221352, \"replayed_location\": 67221352, \"replayed_timestamp\": null, \"paused\": false}, \"timeline\": 2, \"database_system_identifier\": \"7011110722654005156\", \"patroni\": {\"version\": \"2.1.0\", \"scope\": \"stampede1\"}} Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,364 WARNING: Request failed to node3: GET http://node3:8008/patroni (HTTPConnectionPool(host='node3', port=8008): Max retries exceeded with url: /patroni (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f57e06dffa0>, 'Connection to node3 timed out. (connect timeout=2)'))) Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,373 INFO: Software Watchdog activated with 25 second timeout, timing slack 15 seconds Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,385 INFO: promoted self to leader by acquiring session lock Sep 23 14:41:03 node02 patroni[15478]: server promoting Sep 23 14:41:03 node02 patroni[10577]: 2021-09-23 14:41:03,397 INFO: cleared rewind state after becoming the leader Sep 23 14:41:04 node02 patroni[10577]: 2021-09-23 14:41:04,450 INFO: no action. I am (node2) the leader with the lock Sep 23 14:41:04 node02 patroni[10577]: 2021-09-23 14:41:04,475 INFO: no action. I am (node2) the leader with the lock \u2026 \u2026 node2 realizes that the leader is dead, and promotes itself as the leader. Try accessing the cluster using the HAProxy endpoint at any point in time between these operations. The cluster is still accepting connections.","title":"Scenario 2. Abrupt machine shutdown or power outage"},{"location":"solutions/ha-test.html#manual-switchover","text":"Typically, a manual switchover is needed for planned downtime to perform maintenance activity on the leader node. Patroni provides the switchover command to manually switch over from the leader node. Run the following command on node2 (the current leader node): $ sudo patronictl -c /etc/patroni/patroni.yml switchover Patroni asks the name of the current primary node and then the node that should take over as the switched-over primary. You can also specify the time at which the switchover should happen. To trigger the process immediately, specify the value now : primary [node2]: node2 Candidate ['node1', 'node3'] []: node1 When should the switchover take place (e.g. 2021-09-23T15:56 ) [now]: now Current cluster topology + Cluster: stampede1 (7011110722654005156) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Replica | running | 3 | 0 | | node2 | node2 | Leader | running | 3 | | | node3 | node3 | Replica | stopped | | unknown | +--------+-------+---------+---------+----+-----------+ Are you sure you want to switchover cluster stampede1, demoting current primary node2? [y/N]: y 2021-09-23 14:56:40.54009 Successfully switched over to \"node1\" + Cluster: stampede1 (7011110722654005156) -----------+ | Member | Host | Role | State | TL | Lag in MB | +--------+-------+---------+---------+----+-----------+ | node1 | node1 | Leader | running | 3 | | | node2 | node2 | Replica | stopped | | unknown | | node3 | node3 | Replica | stopped | | unknown | +--------+-------+---------+---------+----+-----------+ Restart the Patroni service in node2 (after the \u201cplanned maintenance\u201d). The node rejoins the cluster as a secondary.","title":"Manual switchover"},{"location":"solutions/high-availability.html","text":"High Availability in PostgreSQL with Patroni \u00b6 Summary Solution overview Cluster deployment Testing the cluster PostgreSQL has been widely adopted as a modern, high-performance transactional database. A highly available PostgreSQL cluster can withstand failures caused by network outages, resource saturation, hardware failures, operating system crashes, or unexpected reboots. Such cluster is often a critical component of the enterprise application landscape, where four nines of availability is a minimum requirement. This document provides instructions on how to set up and test a highly-available, single-primary, three-node cluster with Percona PostgreSQL and Patroni . High availability overview There are a few methods for achieving high availability with PostgreSQL: shared disk failover, file system replication, trigger-based replication, statement-based replication, logical replication, and Write-Ahead Log (WAL) shipping. In recent times, PostgreSQL high availability is most commonly achieved with streaming replication . Streaming replication \u00b6 Streaming replication is part of Write-Ahead Log shipping, where changes to the WALs are immediately made available to standby replicas. With this approach, a standby instance is always up-to-date with changes from the primary node and can assume the role of primary in case of a failover. Why native streaming replication is not enough \u00b6 Although the native streaming replication in PostgreSQL supports failing over to the primary node, it lacks some key features expected from a truly highly-available solution. These include: No consensus-based promotion of a \u201cleader\u201d node during a failover No decent capability for monitoring cluster status No automated way to bring back the failed primary node to the cluster A manual or scheduled switchover is not easy to manage To address these shortcomings, there are a multitude of third-party, open-source extensions for PostgreSQL. The challenge for a database administrator here is to select the right utility for the current scenario. Percona Distribution for PostgreSQL solves this challenge by providing the Patroni extension for achieving PostgreSQL high availability. Patroni \u00b6 Patroni provides a template-based approach to create highly available PostgreSQL clusters. Running atop the PostgreSQL streaming replication process, it integrates with watchdog functionality to detect failed primary nodes and take corrective actions to prevent outages. Patroni also provides a pluggable configuration store to manage distributed, multi-node cluster configuration and comes with REST APIs to monitor and manage the cluster. There is also a command-line utility called patronictl that helps manage switchovers and failure scenarios. Architecture layout \u00b6 The following diagram shows the architecture of a three-node PostgreSQL cluster with a single-leader node. Components \u00b6 The following are the components: Three PosgreSQL nodes: node1 , node2 and node3 A dedicated HAProxy node HAProxy-demo . HAProxy is an open-source load balancing software through which client connections to the cluster are routed. ETCD - a distributed configuration storage Softdog - a watchdog utility which is used to detect unhealthy nodes in an acceptable time frame. Deployment \u00b6 Use the links below to navigate to the setup instructions relevant to your operating system Deploy on Debian or Ubuntu Deploy on Red Hat Enterprise Linux or CentOS Testing \u00b6 See the Testing PostgreSQL cluster for the guidelines on how to test your PostgreSQL cluster for replication, failure, switchover.","title":"High availability"},{"location":"solutions/high-availability.html#high-availability-in-postgresql-with-patroni","text":"Summary Solution overview Cluster deployment Testing the cluster PostgreSQL has been widely adopted as a modern, high-performance transactional database. A highly available PostgreSQL cluster can withstand failures caused by network outages, resource saturation, hardware failures, operating system crashes, or unexpected reboots. Such cluster is often a critical component of the enterprise application landscape, where four nines of availability is a minimum requirement. This document provides instructions on how to set up and test a highly-available, single-primary, three-node cluster with Percona PostgreSQL and Patroni . High availability overview There are a few methods for achieving high availability with PostgreSQL: shared disk failover, file system replication, trigger-based replication, statement-based replication, logical replication, and Write-Ahead Log (WAL) shipping. In recent times, PostgreSQL high availability is most commonly achieved with streaming replication .","title":"High Availability in PostgreSQL with Patroni"},{"location":"solutions/high-availability.html#streaming-replication","text":"Streaming replication is part of Write-Ahead Log shipping, where changes to the WALs are immediately made available to standby replicas. With this approach, a standby instance is always up-to-date with changes from the primary node and can assume the role of primary in case of a failover.","title":"Streaming replication"},{"location":"solutions/high-availability.html#why-native-streaming-replication-is-not-enough","text":"Although the native streaming replication in PostgreSQL supports failing over to the primary node, it lacks some key features expected from a truly highly-available solution. These include: No consensus-based promotion of a \u201cleader\u201d node during a failover No decent capability for monitoring cluster status No automated way to bring back the failed primary node to the cluster A manual or scheduled switchover is not easy to manage To address these shortcomings, there are a multitude of third-party, open-source extensions for PostgreSQL. The challenge for a database administrator here is to select the right utility for the current scenario. Percona Distribution for PostgreSQL solves this challenge by providing the Patroni extension for achieving PostgreSQL high availability.","title":"Why native streaming replication is not enough"},{"location":"solutions/high-availability.html#patroni","text":"Patroni provides a template-based approach to create highly available PostgreSQL clusters. Running atop the PostgreSQL streaming replication process, it integrates with watchdog functionality to detect failed primary nodes and take corrective actions to prevent outages. Patroni also provides a pluggable configuration store to manage distributed, multi-node cluster configuration and comes with REST APIs to monitor and manage the cluster. There is also a command-line utility called patronictl that helps manage switchovers and failure scenarios.","title":"Patroni"},{"location":"solutions/high-availability.html#architecture-layout","text":"The following diagram shows the architecture of a three-node PostgreSQL cluster with a single-leader node.","title":"Architecture layout"},{"location":"solutions/high-availability.html#components","text":"The following are the components: Three PosgreSQL nodes: node1 , node2 and node3 A dedicated HAProxy node HAProxy-demo . HAProxy is an open-source load balancing software through which client connections to the cluster are routed. ETCD - a distributed configuration storage Softdog - a watchdog utility which is used to detect unhealthy nodes in an acceptable time frame.","title":"Components"},{"location":"solutions/high-availability.html#deployment","text":"Use the links below to navigate to the setup instructions relevant to your operating system Deploy on Debian or Ubuntu Deploy on Red Hat Enterprise Linux or CentOS","title":"Deployment"},{"location":"solutions/high-availability.html#testing","text":"See the Testing PostgreSQL cluster for the guidelines on how to test your PostgreSQL cluster for replication, failure, switchover.","title":"Testing"}]}